%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Gerko Vink at 2022-09-20 15:58:51 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@article{robins1997,
  title={Non-response models for the analysis of non-monotone ignorable missing data},
  author={Robins, James M and Gill, Richard D},
  journal={Statistics in medicine},
  volume={16},
  number={1},
  pages={39--56},
  year={1997},
  publisher={Wiley Online Library}
}

@article{bart2015,
	author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R and Alzheimer's Disease Neuroimaging Initiative*},
	journal = {Statistical methods in medical research},
	number = {4},
	pages = {462--487},
	publisher = {Sage Publications Sage UK: London, England},
	title = {Multiple imputation of covariates by fully conditional specification: accommodating the substantive model},
	volume = {24},
	year = {2015}}

@article{caiPPC,
	author = {Cai, Mingyang and van Buuren, Stef and Vink, Gerko},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2208.12929},
	journal = {arXiv preprint. arXiv:2208.12929},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Graphical and numerical diagnostic tools to assess multiple imputation models by posterior predictive checking},
	url = {https://arxiv.org/abs/2208.12929},
	year = {2022},
	Bdsk-Url-1 = {https://arxiv.org/abs/2208.12929},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2208.12929}}

@article{amelia,
	author = {James Honaker and Gary King and Matthew Blackwell},
	journal = {Journal of Statistical Software},
	number = {7},
	pages = {1--47},
	title = {{Amelia II}: A Program for Missing Data},
	url = {https://www.jstatsoft.org/v45/i07/},
	volume = {45},
	year = {2011},
	Bdsk-Url-1 = {https://www.jstatsoft.org/v45/i07/}}

@article{abayomi2008diagnostics,
	author = {Abayomi, Kobi and Gelman, Andrew and Levy, Marc},
	date-added = {2016-02-05 21:01:33 +0000},
	date-modified = {2016-02-05 21:01:33 +0000},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\YHCKE6CH\\Abayomi e.a. - 2008 - Diagnostics for multivariate imputations.pdf},
	ids = {abay08},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	number = {3},
	pages = {273--291},
	publisher = {{Wiley Online Library}},
	title = {Diagnostics for Multivariate Imputations},
	volume = {57},
	year = {2008}}

@article{alkire2015global,
	author = {Alkire, Blake C and Raykar, Nakul P and Shrime, Mark G and Weiser, Thomas G and Bickler, Stephen W and Rose, John A and Nutt, Cameron T and Greenberg, Sarah LM and Kotagal, Meera and Riesel, Johanna N and others},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {The Lancet Global Health},
	number = {6},
	pages = {e316--e323},
	publisher = {{Elsevier}},
	title = {Global Access to Surgical Care: A Modelling Study},
	volume = {3},
	year = {2015}}

@article{allan2015determinants,
	author = {Allan, Stephen and Forder, Julien},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Health economics},
	number = {S1},
	pages = {132--145},
	publisher = {{Wiley Online Library}},
	title = {The Determinants of Care Home Closure},
	volume = {24},
	year = {2015}}

@article{ampute,
	abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
	author = {Schouten, Rianne Margaretha and Lugtig, Peter and Vink, Gerko},
	doi = {10.1080/00949655.2018.1491577},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\7UTD6JHS\\Schouten et al. - 2018 - Generating missing values for simulation purposes.pdf;C\:\\Users\\4216318\\Zotero\\storage\\KMSYBXTC\\00949655.2018.html},
	issn = {0094-9655, 1563-5163},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {evaluation,Missing data,multiple imputation,multivariate amputation},
	langid = {english},
	month = oct,
	number = {15},
	pages = {2909--2930},
	publisher = {{Taylor \& Francis}},
	shorttitle = {Generating Missing Values for Simulation Purposes},
	title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure},
	volume = {88},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1080/00949655.2018.1491577}}

@article{bond16,
	abstract = {Multiple imputation has become a popular approach for analyzing incomplete data. Many software packages are available to multiply impute the missing values and to analyze the resulting completed data sets. However, diagnostic tools to check the validity of the imputations are limited, and the majority of the currently available methods need considerable knowledge of the imputation model. In many practical settings, however, the imputer and the analyst may be different individuals or from different organizations, and the analyst model may or may not be congenial to the model used by the imputer. This article develops and evaluates a set of graphical and numerical diagnostic tools for two practical purposes: (i) for an analyst to determine whether the imputations are reasonable under his/her model assumptions without actually knowing the imputation model assumptions; and (ii) for an imputer to fine tune the imputation model by checking the key characteristics of the observed and imputed values. The tools are based on the numerical and graphical comparisons of the distributions of the observed and imputed values conditional on the propensity of response. The methodology is illustrated using simulated data sets created under a variety of scenarios. The examples focus on continuous and binary variables, but the principles can be used to extend methods for other types of variables. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
	author = {Bondarenko, Irina and Raghunathan, Trivellore},
	copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
	doi = {10.1002/sim.6926},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\BBYIKCGA\\Bondarenko en Raghunathan - 2016 - Graphical and numerical diagnostic tools to assess.pdf;C\:\\Users\\4216318\\Zotero\\storage\\Q53PV2BZ\\sim.html},
	issn = {1097-0258},
	journal = {Statistics in Medicine},
	keywords = {congeniality,diagnostics,multiple imputation,propensity score},
	langid = {english},
	number = {17},
	pages = {3007--3020},
	title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models},
	volume = {35},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1002/sim.6926}}

@article{boulesteixIntroductionStatisticalSimulations2020,
	abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
	author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenf{\"u}hrer, J{\"o}rg and Sauerbrei, Willi},
	chapter = {Epidemiology},
	copyright = {\textcopyright{} Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
	doi = {10.1136/bmjopen-2020-039921},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\HHZFFINH\\Boulesteix et al. - 2020 - Introduction to statistical simulations in health .pdf;C\:\\Users\\4216318\\Zotero\\storage\\6APLWJIV\\e039921.html},
	issn = {2044-6055, 2044-6055},
	journal = {BMJ Open},
	keywords = {epidemiology,protocols \& guidelines,statistics \& research methods},
	langid = {english},
	month = dec,
	number = {12},
	pages = {e039921},
	pmid = {33318113},
	publisher = {{British Medical Journal Publishing Group}},
	title = {Introduction to Statistical Simulations in Health Research},
	volume = {10},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1136/bmjopen-2020-039921}}

@phdthesis{brand1999development,
	author = {Brand, Jaap},
	date-added = {2016-02-05 17:56:37 +0000},
	date-modified = {2016-02-05 20:18:12 +0000},
	school = {Erasmus University Rotterdam},
	title = {Development, Implementation and Evaluation of Multiple Imputation Strategies for the Statistical Analysis of Incomplete Data Sets},
	year = {1999}}

@article{buur06,
	abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
	annotation = {\_eprint: https://doi.org/10.1080/10629360600810434},
	author = {Buuren, S. Van and Brand, J. P. L. and {Groothuis-Oudshoorn}, C. G. M. and Rubin, D. B.},
	date-added = {2016-02-05 17:56:23 +0000},
	date-modified = {2016-02-05 17:56:23 +0000},
	doi = {10.1080/10629360600810434},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\YQ2Q2H3J\\Buuren e.a. - 2006 - Fully conditional specification in multivariate im.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5VUVSNA2\\10629360600810434.html},
	ids = {van2006fully},
	issn = {0094-9655},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation},
	month = dec,
	number = {12},
	pages = {1049--1064},
	publisher = {{Taylor \& Francis}},
	title = {Fully Conditional Specification in Multivariate Imputation},
	volume = {76},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1080/10629360600810434}}

@book{buur18,
	author = {Van Buuren, Stef},
	publisher = {{Chapman and Hall/CRC}},
	title = {Flexible Imputation of Missing Data},
	year = {2018}}

@article{crowley2014flexible,
	author = {Crowley, Jocelyn Elise and Kolenikov, Stanislav},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {The Sociological Quarterly},
	number = {1},
	pages = {168--195},
	publisher = {{Wiley Online Library}},
	title = {Flexible Work Options and Mothers' Perceptions of Career Harm},
	volume = {55},
	year = {2014}}

@article{dafoe2013democratic,
	author = {Dafoe, Allan and Oneal, John R and Russett, Bruce},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {International Studies Quarterly},
	number = {1},
	pages = {201--214},
	publisher = {{Wiley Online Library}},
	title = {The Democratic Peace: {{Weighing}} the Evidence and Cautious Inference},
	volume = {57},
	year = {2013}}

@book{DarkData2020,
	abstract = {A practical guide to making good decisions in a world of missing data},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\QKB8XA26\\2020 - Dark Data.pdf;C\:\\Users\\4216318\\Zotero\\storage\\CYWCHLP6\\dark-data.html},
	isbn = {978-0-691-18237-7},
	langid = {english},
	title = {Dark {{Data}}},
	year = {Tue, 02/18/2020 - 12:00}}

@article{de2014religiosity,
	author = {De Hoon, Sean and Van Tubergen, Frank},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {European sociological review},
	pages = {jcu038},
	publisher = {{Oxford Univ Press}},
	title = {The Religiosity of Children of Immigrants and Natives in England, Germany, and the Netherlands: {{The}} Role of Parents and Peers in Class},
	year = {2014}}

@article{dore18,
	abstract = {Recent work (Seaman et al., ; Mealli \& Rubin, ) attempts to clarify the not always well-understood difference between realised and everywhere definitions of missing at random (MAR) and missing completely at random. Another branch of the literature (Mohan et al., ; Pearl \& Mohan, ) exploits always-observed covariates to give variable-based definitions of MAR and missing completely at random. In this paper, we develop a unified taxonomy encompassing all approaches. In this taxonomy, the new concept of `complementary MAR' is introduced, and its relationship with the concept of data observed at random is discussed. All relationships among these definitions are analysed and represented graphically. Conditional independence, both at the random variable and at the event level, is the formal language we adopt to connect all these definitions. Our paper covers both the univariate and the multivariate case, where attention is paid to monotone missingness and to the concept of sequential MAR. Specifically, for monotone missingness, we propose a sequential MAR definition that might be more appropriate than both everywhere and variable-based MAR to model dropout in certain contexts.},
	annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12242},
	author = {Doretti, Marco and Geneletti, Sara and Stanghellini, Elena},
	doi = {10.1111/insr.12242},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\VNFH8XKF\\Doretti et al. - 2018 - Missing Data A Unified Taxonomy Guided by Conditi.pdf;C\:\\Users\\4216318\\Zotero\\storage\\GYINQWHM\\insr.html},
	issn = {1751-5823},
	journal = {International Statistical Review},
	keywords = {conditional independence,dropout,missing data,taxonomy},
	langid = {english},
	number = {2},
	pages = {189--204},
	shorttitle = {Missing {{Data}}},
	title = {Missing {{Data}}: {{A Unified Taxonomy Guided}} by {{Conditional Independence}}},
	volume = {86},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1111/insr.12242}}

@book{fimd,
	author = {Van Buuren, Stef},
	date-added = {2016-02-04 22:35:13 +0000},
	date-modified = {2016-02-04 22:35:43 +0000},
	publisher = {{CRC press}},
	title = {Flexible Imputation of Missing Data},
	year = {2012}}

@article{gree12,
	abstract = {Values influence choice of methodology and thus influence every risk assessment and inference. To deal with this inescapable reality, we need to replace vague and unattainable calls for objectivity with more precise operational qualities. Among qualities that seem widely valued are transparency (openness) and neutrality (balance, fairness). Conformity of researchers to these qualities may be evaluated by considering whether their reports disclose key information desired by readers and whether their methodology encourages initial neutrality among hypotheses of concern. A case study is given in which two authors appearing to share these values and writing on ostensibly the same issues (disclosure and methodology) nonetheless appear to have very different concepts of what the values entail in practice. Thus, more precision is needed in explicating and implementing such values.},
	author = {Greenland, Sander},
	chapter = {Essay},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com.proxy.library.uu.nl/group/rights-licensing/permissions},
	doi = {10.1136/jech-2011-200459},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\JWF9K692\\Greenland - 2012 - Transparency and disclosure, neutrality and balanc.pdf;C\:\\Users\\4216318\\Zotero\\storage\\29EG9WI8\\967.html},
	issn = {0143-005X, 1470-2738},
	journal = {J Epidemiol Community Health},
	keywords = {Conflict of interest,disclosure,epidemiology,ethics,methodology,sociology of knowledge},
	langid = {english},
	month = nov,
	number = {11},
	pages = {967--970},
	pmid = {22268131},
	publisher = {{BMJ Publishing Group Ltd}},
	shorttitle = {Transparency and Disclosure, Neutrality and Balance},
	title = {Transparency and Disclosure, Neutrality and Balance: Shared Values or Just Shared Words?},
	volume = {66},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1136/jech-2011-200459}}

@article{gree17,
	abstract = {There is no complete solution for the problem of abuse of statistics, but methodological training needs to cover cognitive biases and other psychosocial factors affecting inferences. The present paper discusses 3 common cognitive distortions: 1) dichotomania, the compulsion to perceive quantities as dichotomous even when dichotomization is unnecessary and misleading, as in inferences based on whether a P value is "statistically significant"; 2) nullism, the tendency to privilege the hypothesis of no difference or no effect when there is no scientific basis for doing so, as when testing only the null hypothesis; and 3) statistical reification, treating hypothetical data distributions and statistical models as if they reflect known physical laws rather than speculative assumptions for thought experiments. As commonly misused, null-hypothesis significance testing combines these cognitive problems to produce highly distorted interpretation and reporting of study results. Interval estimation has so far proven to be an inadequate solution because it involves dichotomization, an avenue for nullism. Sensitivity and bias analyses have been proposed to address reproducibility problems (Am J Epidemiol. 2017;186(6):646-647); these methods can indeed address reification, but they can also introduce new distortions via misleading specifications for bias parameters. P values can be reframed to lessen distortions by presenting them without reference to a cutoff, providing them for relevant alternatives to the null, and recognizing their dependence on all assumptions used in their computation; they nonetheless require rescaling for measuring evidence. I conclude that methodological development and training should go beyond coverage of mechanistic biases (e.g., confounding, selection bias, measurement error) to cover distortions of conclusions produced by statistical methods and psychosocial forces.},
	author = {Greenland, Sander},
	doi = {10.1093/aje/kwx259},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\JXDA474N\\Greenland - 2017 - Invited Commentary The Need for Cognitive Science.pdf},
	issn = {1476-6256},
	journal = {American Journal of Epidemiology},
	keywords = {behavioral economics,bias analysis,cognitive bias,Cognitive Science,Humans,Models; Statistical,motivated reasoning,nullism,overconfidence,Reproducibility of Results,Research Design,Selection Bias,sensitivity analysis,significance testing},
	langid = {english},
	month = sep,
	number = {6},
	pages = {639--645},
	pmid = {28938712},
	shorttitle = {Invited {{Commentary}}},
	title = {Invited {{Commentary}}: {{The Need}} for {{Cognitive Science}} in {{Methodology}}},
	volume = {186},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1093/aje/kwx259}}

@article{heDiagnosingImputationModels2012a,
	abstract = {Multiple imputation fills in missing data with posterior predictive draws from imputation models. To assess the adequacy of imputation models, we can compare completed data with their replicates simulated under the imputation model. We apply analyses of substantive interest to both datasets and use posterior predictive checks of the differences of these estimates to quantify the evidence of model inadequacy. We can further integrate out the imputed missing data and their replicates over the completed-data analyses to reduce variance in the comparison. In many cases, the checking procedure can be easily implemented using standard imputation software by treating re-imputations under the model as posterior predictive replicates. Thus, it can be applied for non-Bayesian imputation methods. We also sketch several strategies for applying the method in the context of practical imputation analyses. We illustrate the method using two real data applications and study its property using a simulation.},
	author = {He, Yulei and Zaslavsky, Alan M.},
	doi = {10.1002/sim.4413},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\IUZ7LF8I\\He and Zaslavsky - 2012 - Diagnosing imputation models by applying target an.pdf},
	issn = {1097-0258},
	journal = {Statistics in Medicine},
	keywords = {Antineoplastic Agents,Bayes Theorem,Computer Simulation,Data Interpretation; Statistical,Humans,Male,Models; Statistical,Multiple Myeloma,Prognosis,Randomized Controlled Trials as Topic,Sequence Deletion,Survival Analysis},
	langid = {english},
	month = jan,
	number = {1},
	pages = {1--18},
	pmcid = {PMC4233994},
	pmid = {22139814},
	title = {Diagnosing Imputation Models by Applying Target Analyses to Posterior Replicates of Completed Data},
	volume = {31},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1002/sim.4413}}

@article{hoffmannMultiplicityAnalysisStrategies,
	abstract = {For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.},
	author = {Hoffmann, Sabine and Sch{\"o}nbrodt, Felix and Elsas, Ralf and Wilson, Rory and Strasser, Ulrich and Boulesteix, Anne-Laure},
	doi = {10.1098/rsos.201925},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\XVK6YARR\\Hoffmann et al. - The multiplicity of analysis strategies jeopardize.pdf},
	journal = {Royal Society Open Science},
	keywords = {interdisciplinary perspective,metaresearch,open science,replicability crisis,uncertainty},
	number = {4},
	pages = {201925},
	publisher = {{Royal Society}},
	shorttitle = {The Multiplicity of Analysis Strategies Jeopardizes Replicability},
	title = {The Multiplicity of Analysis Strategies Jeopardizes Replicability: Lessons Learned across Disciplines},
	volume = {8},
	Bdsk-Url-1 = {https://doi.org/10.1098/rsos.201925}}

@article{klausch2015selection,
	author = {Klausch, Thomas and Hox, Joop and Schouten, Barry},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	number = {4},
	pages = {945--961},
	publisher = {{Wiley Online Library}},
	title = {Selection Error in Single-and Mixed Mode Surveys of the {{Dutch}} General Population},
	volume = {178},
	year = {2015}}

@article{li2012imputing,
	author = {Li, Fan and Yu, Yaming and Rubin, Donald B},
	date-added = {2016-02-05 21:34:21 +0000},
	date-modified = {2016-02-05 21:34:21 +0000},
	journal = {Duke University Department of Statistical Science Discussion Paper},
	title = {Imputing Missing Data by Fully Conditional Models: {{Some}} Cautionary Examples and Guidelines},
	volume = {1124},
	year = {2012}}

@book{litt20,
	author = {Little and Rubin},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\7J824PPV\\Statistical Analysis with Missing Data, Third Edit.pdf;C\:\\Users\\4216318\\Zotero\\storage\\V4RFI4LL\\9781119482260.html},
	title = {Statistical {{Analysis}} with {{Missing Data}}, {{Third Edition}} | {{Wiley Series}} in {{Probability}} and {{Statistics}}},
	year = {2020}}

@article{little1988missing,
	author = {Little, Roderick JA},
	date-added = {2016-02-05 18:55:45 +0000},
	date-modified = {2016-02-05 18:55:45 +0000},
	journal = {Journal of Business \& Economic Statistics},
	number = {3},
	pages = {287--296},
	publisher = {{Taylor \& Francis}},
	title = {Missing-Data Adjustments in Large Surveys},
	volume = {6},
	year = {1988}}

@article{little2012calibrated,
	author = {Little, Roderick J},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Journal of Official Statistics},
	number = {3},
	pages = {309},
	title = {Calibrated {{Bayes}}, an Alternative Inferential Paradigm for Official Statistics},
	volume = {28},
	year = {2012}}

@article{little2012prevention,
	author = {Little, Roderick J and D'Agostino, Ralph and Cohen, Michael L and Dickersin, Kay and Emerson, Scott S and Farrar, John T and Frangakis, Constantine and Hogan, Joseph W and Molenberghs, Geert and Murphy, Susan A and others},
	date-added = {2016-01-31 18:40:06 +0000},
	date-modified = {2016-01-31 18:40:06 +0000},
	journal = {New England Journal of Medicine},
	number = {14},
	pages = {1355--1360},
	publisher = {{Mass Medical Soc}},
	title = {The Prevention and Treatment of Missing Data in Clinical Trials},
	volume = {367},
	year = {2012}}

@incollection{liu21,
	abstract = {This chapter addresses important steps during the quality assurance and control of RWD, with particular emphasis on the identification and handling of missing values. A gentle introduction is provided on common statistical and machine learning methods for imputation. We discuss the main strengths and weaknesses of each method, and compare their performance in a literature review. We motivate why the imputation of RWD may require additional efforts to avoid bias, and highlight recent advances that account for informative missingness and repeated observations. Finally, we introduce alternative methods to address incomplete data without the need for imputation.},
	archiveprefix = {arXiv},
	author = {Liu, Dawei and Oberman, Hanne I. and Mu{\~n}oz, Johanna and Hoogland, Jeroen and Debray, Thomas P. A.},
	booktitle = {Clinical Applications of Artificial Intelligence in Real-World Data},
	eprint = {2110.15877},
	eprinttype = {arxiv},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\2THQ48KE\\Liu et al. - 2021 - Quality control, data cleaning, imputation.pdf;C\:\\Users\\4216318\\Zotero\\storage\\9PK9I7SV\\2110.html},
	keywords = {62D10,G.3,I.5.1,J.3,Statistics - Methodology},
	month = oct,
	title = {Quality Control, Data Cleaning, Imputation},
	year = {2021}}

@article{meal15,
	abstract = {We clarify the key concept of missingness at random in incomplete data analysis. We first distinguish between data being missing at random and the missingness mechanism being a missing-at-random one, which we call missing always at random and which is more restrictive. We further discuss how, in general, neither of these conditions is a statement about conditional independence. We then consider the implication of the more restrictive missing-always-at-random assumption when coupled with full unit-exchangeability for the matrix of the variables of interest and the missingness indicators: the conditional distribution of the missingness indicators for any variable that can have a missing value can depend only on variables that are always fully observed. We discuss implications of this for modelling missingness mechanisms.},
	author = {Mealli, Fabrizia and Rubin, Donald B.},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\KF4JBHVR\\MEALLI and RUBIN - 2015 - Clarifying missing at random and related definitio.pdf},
	issn = {0006-3444},
	journal = {Biometrika},
	number = {4},
	pages = {995--1000},
	publisher = {{[Oxford University Press, Biometrika Trust]}},
	title = {Clarifying Missing at Random and Related Definitions, and Implications When Coupled with Exchangeability},
	volume = {102},
	year = {2015}}

@article{meng94,
	abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
	author = {Meng, Xiao-Li},
	date-added = {2016-02-04 19:29:48 +0000},
	date-modified = {2016-02-04 23:48:01 +0000},
	doi = {10.1214/ss/1177010269},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\LDSDEUAS\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\4216318\\Zotero\\storage\\PHA35LD7\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MNNUQRPL\\1177010269.html},
	ids = {meng1994multiple},
	issn = {0883-4237, 2168-8745},
	journal = {Statistical Science},
	keywords = {Congeniality,importance sampling,incomplete data,missing data,nonresponse,normalizing constants,public-use data file,randomization,self-efficiency},
	langid = {english},
	month = nov,
	number = {4},
	pages = {538--558},
	publisher = {{JSTOR}},
	title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
	volume = {9},
	year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1214/ss/1177010269}}

@article{mice,
	author = {Van Buuren, Stef and {Groothuis-Oudshoorn}, Karin},
	copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
	date-added = {2016-02-04 23:31:15 +0000},
	date-modified = {2016-02-04 23:32:22 +0000},
	doi = {10.18637/jss.v045.i03},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\W7DXJIVB\\Buuren en Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\4216318\\Zotero\\storage\\6GARXLG6\\v045i03.html},
	journal = {Journal of Statistical Software},
	langid = {english},
	month = dec,
	number = {1},
	pages = {1--67},
	publisher = {{American Statistical Association}},
	shorttitle = {Mice},
	title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
	volume = {45},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.18637/jss.v045.i03}}

@article{moha21,
	abstract = {This article reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We first examine the limitations of traditional frameworks from three different perspectives: transparency, estimability, and testability. We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are missing not at random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally, we derive testable implications for missing data models in both missing at random and MNAR categories.},
	annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1874961},
	author = {Mohan, Karthika and Pearl, Judea},
	doi = {10.1080/01621459.2021.1874961},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\ST2AY3Q4\\Mohan and Pearl - 2021 - Graphical Models for Processing Missing Data.pdf;C\:\\Users\\4216318\\Zotero\\storage\\LMRAVINF\\01621459.2021.html},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {Graphical models,Missing data,Missing not at random,Nonignorable,Recoverability,Testability},
	month = apr,
	number = {534},
	pages = {1023--1037},
	publisher = {{Taylor \& Francis}},
	title = {Graphical {{Models}} for {{Processing Missing Data}}},
	volume = {116},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2021.1874961}}

@article{molenberghs2008every,
	author = {Molenberghs, Geert and Beunckens, Caroline and Sotto, Cristina and Kenward, Michael G},
	date-added = {2016-02-04 19:19:43 +0000},
	date-modified = {2016-02-04 19:19:43 +0000},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {2},
	pages = {371--388},
	publisher = {{Wiley Online Library}},
	title = {Every Missingness Not at Random Model Has a Missingness at Random Counterpart with Equal Fit},
	volume = {70},
	year = {2008}}

@book{molenberghs2014handbook,
	author = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G and Tsiatis, Anastasios and Verbeke, Geert},
	date-added = {2016-02-05 13:36:42 +0000},
	date-modified = {2016-02-05 13:36:42 +0000},
	publisher = {{CRC Press}},
	title = {Handbook of Missing Data Methodology},
	year = {2014}}

@article{more18,
	abstract = {With incomplete data, the "missing at random" (MAR) assumption is widely understood to enable unbiased estimation with appropriate methods. While the need to assess the plausibility of MAR and to perform sensitivity analyses considering "missing not at random" (MNAR) scenarios has been emphasized, the practical difficulty of these tasks is rarely acknowledged. With multivariable missingness, what MAR means is difficult to grasp, and in many MNAR scenarios unbiased estimation is possible using methods commonly associated with MAR. Directed acyclic graphs (DAGs) have been proposed as an alternative framework for specifying practically accessible assumptions beyond the MAR-MNAR dichotomy. However, there is currently no general algorithm for deciding how to handle the missing data given a specific DAG. Here we construct "canonical" DAGs capturing typical missingness mechanisms in epidemiologic studies with incomplete data on exposure, outcome, and confounding factors. For each DAG, we determine whether common target parameters are "recoverable," meaning that they can be expressed as functions of the available data distribution and thus estimated consistently, or whether sensitivity analyses are necessary. We investigate the performance of available-case and multiple-imputation procedures. Using data from waves 1-3 of the Longitudinal Study of Australian Children (2004-2008), we illustrate how our findings can guide the treatment of missing data in point-exposure studies.},
	author = {{Moreno-Betancur}, Margarita and Lee, Katherine J. and Leacy, Finbarr P. and White, Ian R. and Simpson, Julie A. and Carlin, John B.},
	doi = {10.1093/aje/kwy173},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\4XV8YW8E\\Moreno-Betancur et al. - 2018 - Canonical Causal Diagrams to Guide the Treatment o.pdf},
	issn = {1476-6256},
	journal = {American Journal of Epidemiology},
	keywords = {Algorithms,Data Interpretation; Statistical,Epidemiologic Methods,Humans,Longitudinal Studies},
	langid = {english},
	month = dec,
	number = {12},
	pages = {2705--2715},
	pmcid = {PMC6269242},
	pmid = {30124749},
	title = {Canonical {{Causal Diagrams}} to {{Guide}} the {{Treatment}} of {{Missing Data}} in {{Epidemiologic Studies}}},
	volume = {187},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1093/aje/kwy173}}

@article{morr18,
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods...},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	doi = {10.1002/sim.8086},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\2WCQMEXM\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\C2X36276\\Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\EPPH7J47\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\IHBUZDPP\\sim.html;C\:\\Users\\4216318\\Zotero\\storage\\IKXB5QMM\\sim.html;C\:\\Users\\4216318\\Zotero\\storage\\LN3P265W\\login.html},
	ids = {morrisUsingSimulationStudies2019},
	issn = {1097-0258},
	journal = {Statistics in Medicine},
	keywords = {graphics for simulation,Monte Carlo,simulation design,simulation reporting,simulation studies},
	langid = {english},
	month = may,
	number = {11},
	pages = {2074--2102},
	publisher = {{John Wiley \& Sons, Ltd}},
	title = {Using Simulation Studies to Evaluate Statistical Methods},
	volume = {38},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1002/sim.8086}}

@article{neym34,
	author = {Neyman, Jerzy},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	doi = {10.2307/2342192},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\ZMX9KJKR\\Neyman - 1934 - On the Two Different Aspects of the Representative.pdf},
	ids = {neyman1934two},
	journal = {Journal of the Royal Statistical Society},
	number = {4},
	pages = {558--625},
	publisher = {{JSTOR}},
	title = {On the {{Two Different Aspects}} of the {{Representative Method}}: {{The Method}} of {{Stratified Sampling}} and the {{Method}} of {{Purposive Selection}}},
	volume = {97},
	year = {1934},
	Bdsk-Url-1 = {https://doi.org/10.2307/2342192}}

@article{nguy17,
	abstract = {Background:\hspace{0.6em} Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models. Analysis:\hspace{0.6em} In this paper, we provide an overview of currently available methods for checking imputation models. These include graphical checks and numerical summaries, as well as simulation-based methods such as posterior predictive checking. These model checking techniques are illustrated using an analysis affected by missing data from the Longitudinal Study of Australian Children. Conclusions:\hspace{0.6em} As multiple imputation becomes further established as a standard approach for handling missing data, it will become increasingly important that researchers employ appropriate model checking approaches to ensure that reliable results are obtained when using this method.},
	author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
	doi = {10.1186/s12982-017-0062-6},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\292299YB\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\35CSCF29\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\GNUQEAQA\\Nguyen e.a. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TN7MF2IX\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TYVJUJNV\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5E7A46Y4\\s12982-017-0062-6.html;C\:\\Users\\4216318\\Zotero\\storage\\CDXNKIZY\\s12982-017-0062-6.html;C\:\\Users\\4216318\\Zotero\\storage\\LCHBMRXB\\s12982-017-0062-6.html},
	ids = {nguyenModelCheckingMultiple2017a},
	issn = {1742-7622},
	journal = {Emerging Themes in Epidemiology},
	keywords = {Cross-validation,Diagnostics,Missing data,Model checking,Multiple imputation,Posterior predictive checking},
	langid = {english},
	month = dec,
	number = {1},
	pages = {8},
	pmcid = {PMC5569512},
	pmid = {28852415},
	shorttitle = {Model Checking in Multiple Imputation},
	title = {Model Checking in Multiple Imputation: An Overview and Case Study},
	volume = {14},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1186/s12982-017-0062-6}}

@article{niesslOveroptimismBenchmarkStudies2022,
	abstract = {In recent years, the need for neutral benchmark studies that focus on the comparison of methods coming from computational sciences has been increasingly recognized by the scientific community. While general advice on the design and analysis of neutral benchmark studies can be found in recent literature, a certain flexibility always exists. This includes the choice of data sets and performance measures, the handling of missing performance values, and the way the performance values are aggregated over the data sets. As a consequence of this flexibility, researchers may be concerned about how their choices affect the results or, in the worst case, may be tempted to engage in questionable research practices (e.g., the selective reporting of results or the post hoc modification of design or analysis components) to fit their expectations. To raise awareness for this issue, we use an example benchmark study to illustrate how variable benchmark results can be when all possible combinations of a range of design and analysis options are considered. We then demonstrate how the impact of each choice on the results can be assessed using multidimensional unfolding. In conclusion, based on previous literature and on our illustrative example, we claim that the multiplicity of design and analysis options combined with questionable research practices lead to biased interpretations of benchmark results and to over-optimistic conclusions. This issue should be considered by computational researchers when designing and analyzing their benchmark studies and by the scientific community in general in an effort towards more reliable benchmark results. This article is categorized under: Technologies {$>$} Visualization Technologies {$>$} Data Preprocessing Technologies {$>$} Structure Discovery and Clustering},
	annotation = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1441},
	author = {Nie{\ss}l, Christina and Herrmann, Moritz and Wiedemann, Chiara and Casalicchio, Giuseppe and Boulesteix, Anne-Laure},
	doi = {10.1002/widm.1441},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\UAPDWNQF\\Nie{\ss}l et al. - 2022 - Over-optimism in benchmark studies and the multipl.pdf;C\:\\Users\\4216318\\Zotero\\storage\\AMR4YZXI\\widm.html},
	issn = {1942-4795},
	journal = {WIREs Data Mining and Knowledge Discovery},
	keywords = {benchmarking,method comparison,over-optimistic results,questionable research practices,variability of results},
	langid = {english},
	number = {2},
	pages = {e1441},
	title = {Over-Optimism in Benchmark Studies and the Multiplicity of Design and Analysis Options When Interpreting Their Results},
	volume = {12},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1002/widm.1441}}

@article{nijm20,
	abstract = {Abstract                            Aims               Use of prediction models is widely recommended by clinical guidelines, but usually requires complete information on all predictors, which is not always available in daily practice. We aim to describe two methods for real-time handling of missing predictor values when using prediction models in practice.                                         Methods and results               We compare the widely used method of mean imputation (M-imp) to a method that personalizes the imputations by taking advantage of the observed patient characteristics. These characteristics may include both prediction model variables and other characteristics (auxiliary variables). The method was implemented using imputation from a joint multivariate normal model of the patient characteristics (joint modelling imputation; JMI). Data from two different cardiovascular cohorts with cardiovascular predictors and outcome were used to evaluate the real-time imputation methods. We quantified the prediction model's overall performance [mean squared error (MSE) of linear predictor], discrimination (c-index), calibration (intercept and slope), and net benefit (decision curve analysis). When compared with mean imputation, JMI substantially improved the MSE (0.10 vs. 0.13), c-index (0.70 vs. 0.68), and calibration (calibration-in-the-large: 0.04 vs. 0.06; calibration slope: 1.01 vs. 0.92), especially when incorporating auxiliary variables. When the imputation method was based on an external cohort, calibration deteriorated, but discrimination remained similar.                                         Conclusions               We recommend JMI with auxiliary variables for real-time imputation of missing values, and to update imputation models when implementing them in new settings or (sub)populations.},
	author = {Nijman, Steven W J and Hoogland, Jeroen and Groenhof, T Katrien J and Brandjes, Menno and Jacobs, John J L and Bots, Michiel L and Asselbergs, Folkert W and Moons, Karel G M and Debray, Thomas P A},
	date-modified = {2022-09-20 15:55:33 +0200},
	doi = {10.1093/ehjdh/ztaa016},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\HUXBXJ7Y\\Nijman et al. - 2020 - Real-time imputation of missing predictor values i.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MGSRFKTJ\\Nijman et al. - 2021 - Real-time imputation of missing predictor values i.pdf;C\:\\Users\\4216318\\Zotero\\storage\\WHJ9L9MB\\6042147.html},
	ids = {nijmanRealtimeImputationMissing2021a},
	issn = {2634-3916},
	journal = {European Heart Journal - Digital Health},
	langid = {english},
	month = dec,
	title = {Real-Time Imputation of Missing Predictor Values in Clinical Practice},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1093/ehjdh/ztaa016}}

@article{ober21,
	abstract = {Iterative imputation is a popular tool to accommodate missing data. While it is widely accepted that valid inferences can be obtained with this technique, these inferences all rely on algorithmic convergence. There is no consensus on how to evaluate the convergence properties of the method. Our study provides insight into identifying non-convergence in iterative imputation algorithms. We found that--in the cases considered--inferential validity was achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value.},
	archiveprefix = {arXiv},
	author = {Oberman, Hanne I. and {van Buuren}, Stef and Vink, Gerko},
	date-modified = {2022-09-20 13:26:56 +0200},
	eprint = {2110.11951},
	eprinttype = {arxiv},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\8QQ2TQFU\\Oberman et al. - 2021 - Missing the Point Non-Convergence in Iterative Im.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TQRM4LVP\\2110.html},
	journal = {arXiv preprint. arXiv:2110.11951},
	keywords = {Statistics - Applications,Statistics - Computation},
	month = oct,
	shorttitle = {Missing the {{Point}}},
	title = {Missing the {{Point}}: {{Non-Convergence}} in {{Iterative Imputation Algorithms}}},
	year = {2021}}

@article{pawe22,
	abstract = {Comparative simulation studies are workhorse tools for benchmarking statistical methods, but if not performed transparently they may lead to overoptimistic or misleading conclusions. The current publication requirements adopted by statistics journals do not prevent questionable research practices such as selective reporting. The past years have witnessed numerous suggestions and initiatives to improve on these issues but little progress can be seen to date. In this paper we discuss common questionable research practices which undermine the validity of findings from comparative simulation studies. To illustrate our point, we invent a novel prediction method with no expected performance gain and benchmark it in a pre-registered comparative simulation study. We show how easy it is to make the method appear superior over well-established competitor methods if no protocol is in place and various questionable research practices are employed. Finally, we provide researchers, reviewers, and other academic stakeholders with concrete suggestions for improving the methodological quality of comparative simulation studies, most importantly the need for pre-registered simulation protocols.},
	archiveprefix = {arXiv},
	author = {Pawel, Samuel and Kook, Lucas and Reeve, Kelly},
	date-modified = {2022-09-20 15:56:20 +0200},
	eprint = {2203.13076},
	eprinttype = {arxiv},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\RVRASS5R\\Pawel et al. - 2022 - Pitfalls and Potentials in Simulation Studies.pdf;C\:\\Users\\4216318\\Zotero\\storage\\2P7J7YT3\\2203.html},
	journal = {arXiv preprint. arXiv:2110.11951},
	keywords = {Statistics - Computation,Statistics - Methodology},
	month = mar,
	primaryclass = {stat},
	publisher = {{arXiv}},
	title = {Pitfalls and {{Potentials}} in {{Simulation Studies}}},
	year = {2022}}

@article{pete14,
	abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
	author = {Petersen, Maya L. and {van der Laan}, Mark J.},
	doi = {10.1097/EDE.0000000000000078},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\BGQLYCY6\\Petersen and van der Laan - 2014 - Causal Models and Learning from Data Integrating .pdf;C\:\\Users\\4216318\\Zotero\\storage\\SM6PLS5Z\\Causal_Models_and_Learning_from_Data__Integrating.13.html},
	issn = {1044-3983},
	journal = {Epidemiology},
	langid = {american},
	month = may,
	number = {3},
	pages = {418--426},
	shorttitle = {Causal {{Models}} and {{Learning}} from {{Data}}},
	title = {Causal {{Models}} and {{Learning}} from {{Data}}: {{Integrating Causal Modeling}} and {{Statistical Estimation}}},
	volume = {25},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1097/EDE.0000000000000078}}

@article{pulkki2015cumulative,
	author = {{Pulkki-R{\aa}back}, Laura and Elovainio, Marko and Hakulinen, Christian and Lipsanen, Jari and Hintsanen, Mirka and Jokela, Markus and Kubzansky, Laura D and Hintsa, Taina and Serlachius, Anna and Laitinen, Tomi T and others},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Circulation},
	number = {3},
	pages = {245--253},
	publisher = {{Am Heart Assoc}},
	title = {Cumulative Effect of Psychosocial Factors in Youth on Ideal Cardiovascular Health in Adulthood the Cardiovascular Risk in Young Finns Study},
	volume = {131},
	year = {2015}}

@manual{R,
	address = {{Vienna, Austria}},
	author = {Team, R Core},
	date-added = {2016-02-04 23:33:33 +0000},
	date-modified = {2016-02-04 23:35:08 +0000},
	organization = {{R Foundation for Statistical Computing}},
	title = {R: {{A}} Language and Environment for Statistical Computing},
	type = {Manual},
	year = {2015}}

@article{raghunathan2003multiple,
	author = {Raghunathan, Trivellore E and Reiter, Jerome P and Rubin, Donald B},
	date-added = {2016-02-05 21:45:54 +0000},
	date-modified = {2016-02-05 21:45:54 +0000},
	journal = {Journal of Official Statistics},
	number = {1},
	pages = {1},
	publisher = {{Statistics Sweden (SCB)}},
	title = {Multiple Imputation for Statistical Disclosure Limitation},
	volume = {19},
	year = {2003}}

@article{rubi76,
	abstract = {When making sampling distribution inferences about the parameter of the data, \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from \texttheta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
	author = {Rubin, Donald B.},
	date-added = {2016-01-31 19:05:50 +0000},
	date-modified = {2016-01-31 19:05:50 +0000},
	doi = {10.2307/2335739},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\XIZEI53J\\Rubin - 1976 - Inference and Missing Data.pdf},
	ids = {rubin1976inference},
	journal = {Biometrika},
	number = {3},
	pages = {581--592},
	publisher = {{Biometrika Trust}},
	title = {Inference and {{Missing Data}}},
	volume = {63},
	year = {1976},
	Bdsk-Url-1 = {https://doi.org/10.2307/2335739}}

@book{rubi87,
	address = {{New York, NY}},
	author = {Rubin, Donald B.},
	date-added = {2016-01-31 18:37:31 +0000},
	date-modified = {2016-01-31 18:41:54 +0000},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\63UKR3PY\\Rubin - 1987 - Multiple Imputation for nonresponse in surveys.pdf},
	ids = {rubin1987},
	langid = {english},
	publisher = {{Wiley}},
	series = {Wiley Series in Probability and Mathematical Statistics {{Applied}} Probability and Statistics},
	title = {Multiple {{Imputation}} for Nonresponse in Surveys},
	year = {1987}}

@article{rubin1986statistical,
	author = {Rubin, Donald B},
	date-added = {2016-02-05 18:56:47 +0000},
	date-modified = {2016-02-05 18:56:47 +0000},
	journal = {Journal of Business \& Economic Statistics},
	number = {1},
	pages = {87--94},
	publisher = {{Taylor \& Francis Group}},
	title = {Statistical Matching Using File Concatenation with Adjusted Weights and Multiple Imputations},
	volume = {4},
	year = {1986}}

@article{scho18,
	abstract = {Missing data in scientific research go hand in hand with assumptions about the nature of the missingness. When dealing with missing values, a set of beliefs has to be formulated about the extent to which the observed data may also hold for the missing parts of the data. It is vital that the validity of these missingness assumptions is verified, tested, and that assumptions are adjusted when necessary. In this article, we demonstrate how observed data structures could a priori indicate whether it is likely that our beliefs about the missingness can be trusted. To this end, we simulate complete data and generate missing values according several types of MCAR, MAR, and MNAR mechanisms. We demonstrate that in scenarios where the data correlations are either low or very substantial, strictly different mechanisms yield equivalent statistical inferences. In addition, we show that the choice of quantity of scientific interest together with the distribution of the nonresponse govern the validity of the missingness assumptions.},
	author = {Schouten, Rianne Margaretha and Vink, Gerko},
	doi = {10.1177/0049124118799376},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\V8JNSTN7\\Schouten and Vink - 2021 - The Dance of the Mechanisms How Observed Informat.pdf},
	issn = {0049-1241},
	journal = {Sociological Methods \& Research},
	keywords = {missing data methodology,missingness assumptions,multivariate amputation},
	langid = {english},
	month = aug,
	number = {3},
	pages = {1243--1258},
	publisher = {{SAGE Publications Inc}},
	shorttitle = {The {{Dance}} of the {{Mechanisms}}},
	title = {The {{Dance}} of the {{Mechanisms}}: {{How Observed Information Influences}} the {{Validity}} of {{Missingness Assumptions}}},
	volume = {50},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1177/0049124118799376}}

@article{scho21,
	abstract = {The causal effect of an intervention (treatment/exposure) on an outcome can be estimated by: i) specifying knowledge about the data-generating process; ii) assessing under what assumptions a target quantity, such as for example a causal odds ratio, can be identified given the specified knowledge (and given the measured data); and then, iii) using appropriate statistical estimation techniques to estimate the desired parameter of interest. As regression is the cornerstone of statistical analysis, it seems obvious to ask: is it appropriate to use estimated regression parameters for causal effect estimation? It turns out that using regression for effect estimation is possible, but typically requires more assumptions than competing methods. This manuscript provides a comprehensive summary of the assumptions needed to identify and estimate a causal parameter using regression and, equally important, discusses the resulting implications for statistical practice.},
	archiveprefix = {arXiv},
	author = {Schomaker, Michael},
	date-modified = {2022-09-20 15:56:49 +0200},
	doi = {10.48550/arXiv.2006.11754},
	eprint = {2006.11754},
	eprinttype = {arxiv},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\76MND5AX\\Schomaker - 2021 - Regression and Causality.pdf;C\:\\Users\\4216318\\Zotero\\storage\\D88BMLAW\\2006.html},
	journal = {arXiv preprint. arXiv:2006.11754},
	keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
	month = mar,
	primaryclass = {math, stat},
	publisher = {{arXiv}},
	title = {Regression and {{Causality}}},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.48550/arXiv.2006.11754}}

@article{seam13,
	abstract = {The concept of missing at random is central in the literature on statistical analysis with missing data. In general, inference using incomplete data should be based not only on observed data values but should also take account of the pattern of missing values. However, it is often said that if data are missing at random, valid inference using likelihood approaches (including Bayesian) can be obtained ignoring the missingness mechanism. Unfortunately, the term "missing at random" has been used inconsistently and not always clearly; there has also been a lack of clarity around the meaning of "valid inference using likelihood". These issues have created potential for confusion about the exact conditions under which the missingness mechanism can be ignored, and perhaps fed confusion around the meaning of "analysis ignoring the missingness mechanism". Here we provide standardised precise definitions of "missing at random" and "missing completely at random", in order to promote unification of the theory. Using these definitions we clarify the conditions that suffice for "valid inference" to be obtained under a variety of inferential paradigms.},
	author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\EXRR3CWW\\Seaman et al. - 2013 - What Is Meant by Missing at Random.pdf},
	issn = {0883-4237},
	journal = {Statistical Science},
	number = {2},
	pages = {257--268},
	publisher = {{Institute of Mathematical Statistics}},
	title = {What {{Is Meant}} by "{{Missing}} at {{Random}}"?},
	volume = {28},
	year = {2013}}

@article{shara2015randomly,
	author = {Shara, Nawar and Yassin, Sayf A and Valaitis, Eduardas and Wang, Hong and Howard, Barbara V and Wang, Wenyu and Lee, Elisa T and Umans, Jason G},
	date-added = {2016-02-05 20:16:12 +0000},
	date-modified = {2016-02-05 20:16:12 +0000},
	journal = {PloS one},
	number = {9},
	pages = {e0138923},
	publisher = {{Public Library of Science}},
	title = {Randomly and Non-Randomly Missing Renal Function Data in the Strong Heart Study: A Comparison of Imputation Methods},
	volume = {10},
	year = {2015}}

@article{sorbi2014medium,
	author = {Sorbi, MJ and Kleiboer, AM and {van Silfhout}, HG and Vink, G and Passchier, J},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Cephalalgia : an international journal of headache},
	pages = {0333102414547137},
	publisher = {{SAGE Publications}},
	title = {Medium-Term Effectiveness of Online Behavioral Training in Migraine Self-Management: {{A}} Randomized Trial Controlled over 10 Months},
	year = {2014}}

@article{sper20,
	abstract = {Missing data are much studied in epidemiology and statistics. Theoretical development and application of methods for handling missing data have mostly been conducted in the context of prospective research data and with a goal of description or causal explanation. However, it is now common to build predictive models using routinely collected data, where missing patterns may convey important information, and one might take a pragmatic approach to optimizing prediction. Therefore, different methods to handle missing data may be preferred. Furthermore, an underappreciated issue in prediction modeling is that the missing data method used in model development may not match the method used when a model is deployed. This may lead to overoptimistic assessments of model performance. For prediction, particularly with routinely collected data, methods for handling missing data that incorporate information within the missingness pattern should be explored and further developed. Where missing data methods differ between model development and model deployment, the implications of this must be explicitly evaluated. The trade-off between building a prediction model that is causally principled, and building a prediction model that maximizes the use of all available information, should be carefully considered and will depend on the intended use of the model.},
	author = {Sperrin, Matthew and Martin, Glen P. and Sisk, Rose and Peek, Niels},
	doi = {10.1016/j.jclinepi.2020.03.028},
	issn = {1878-5921},
	journal = {Journal of Clinical Epidemiology},
	keywords = {Causality,Clinical prediction models,Data Interpretation; Statistical,Data Management,Humans,Missing data,Model performance,Models; Statistical,Multiple imputation,Prognosis,Prognostic model,Prospective Studies,Routinely collected data},
	langid = {english},
	month = sep,
	pages = {183--187},
	pmid = {32540389},
	title = {Missing Data Should Be Handled Differently for Prediction than for Description or Causal Explanation},
	volume = {125},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jclinepi.2020.03.028}}

@article{stinesen2015reduced,
	author = {Stinesen Kollberg, Karin and Waldenstr{\"o}m, Ann-Charlotte and Bergmark, Karin and Dunberger, Gail and Rossander, Anna and Wilder{\"a}ng, Ulrica and {\AA}vall-Lundqvist, Elisabeth and Steineck, Gunnar},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Acta Oncologica},
	number = {5},
	pages = {772--779},
	publisher = {{Taylor \& Francis}},
	title = {Reduced Vaginal Elasticity, Reduced Lubrication, and Deep and Superficial Dyspareunia in Irradiated Gynecological Cancer Survivors},
	volume = {54},
	year = {2015}}

@article{toCharacterizingEffectsMissing2018,
	abstract = {BACKGROUND: The Toxicological Priority Index (ToxPi) is a method for prioritization and profiling of chemicals that integrates data from diverse sources. However,  individual data sources ("assays"), such as in vitro bioassays or in vivo study  endpoints, often feature sections of missing data, wherein subsets of chemicals have  not been tested in all assays. In order to investigate the effects of missing data  and recommend solutions, we designed simulation studies around high-throughput  screening data generated by the ToxCast and Tox21 programs on chemicals highlighted  by the Agency for Toxic Substances and Disease Registry's (ATSDR) Substance Priority  List (SPL), which helps prioritize environmental research and remediation resources.  RESULTS: Our simulations explored a wide range of scenarios concerning data (0-80\%  assay data missing per chemical), modeling (ToxPi models containing from 160-700  different assays), and imputation method (k-Nearest-Neighbor, Max, Mean, Min,  Binomial, Local Least Squares, and Singular Value Decomposition). We find that most  imputation methods result in significant changes to ToxPi score, except for datasets  with a small number of assays. If we consider rank change conditional on these  significant changes to ToxPi score, we find that ranks of chemicals in the minimum  value imputation, SVD imputation, and kNN imputation sets are more sensitive to the  score changes. CONCLUSIONS: We found that the choice of imputation strategy exerted  significant influence over both scores and associated ranks, and the most sensitive  scenarios were those involving fewer assays plus higher proportions of missing data.  By characterizing the effects of missing data and the relative benefit of imputation  approaches across real-world data scenarios, we can augment confidence in the  robustness of decisions regarding the health and ecological effects of environmental  chemicals.},
	author = {To, Kimberly T. and Fry, Rebecca C. and Reif, David M.},
	doi = {10.1186/s13040-018-0169-5},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\N25MEBT5\\To et al. - 2018 - Characterizing the effects of missing data and eva.pdf;C\:\\Users\\4216318\\Zotero\\storage\\WU34E9ED\\s13040-018-0169-5.html},
	ids = {toCharacterizingEffectsMissing2018a},
	issn = {1756-0381},
	journal = {BioData mining},
	keywords = {Chemical prioritization,Imputation,Missing data,Multiple imputation,Simulation,ToxCast,ToxPi},
	langid = {english},
	pages = {10},
	pmcid = {PMC5998548},
	pmid = {29942350},
	title = {Characterizing the Effects of Missing Data and Evaluating Imputation Methods for Chemical Prioritization Applications Using {{ToxPi}}.},
	volume = {11},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1186/s13040-018-0169-5}}

@article{vink14,
	abstract = {Current pooling rules for multiply imputed data assume infinite populations. In some situations this assumption is not feasible as every unit in the population has been observed, potentially leading to over-covered population estimates. We simplify the existing pooling rules for situations where the sampling variance is not of interest. We compare these rules to the conventional pooling rules and demonstrate their use in a situation where there is no sampling variance. Using the standard pooling rules in situations where sampling variance should not be considered, leads to overestimation of the variance of the estimates of interest, especially when the amount of missingness is not very large. As a result, populations estimates are over-covered, which may lead to a loss of statistical power. We conclude that the theory of multiple imputation can be extended to the situation where the sample happens to be the population. The simplified pooling rules can be easily implemented to obtain valid inference in cases where we have observed essentially all units and in simulation studies addressing the missingness mechanism only.},
	author = {Vink, Gerko and {van Buuren}, Stef},
	date-added = {2016-01-31 18:55:15 +0000},
	date-modified = {2016-01-31 18:55:15 +0000},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\9TV8WDM5\\Vink en van Buuren - 2014 - Pooling multiple imputations when the sample happe.pdf;C\:\\Users\\4216318\\Zotero\\storage\\P6KYWI5H\\1409.html},
	ids = {vink2014pooling},
	keywords = {Mathematics - Statistics Theory,Statistics - Computation},
	month = sep,
	title = {Pooling Multiple Imputations When the Sample Happens to Be the Population},
	year = {2014}}

@article{vinknd,
	abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
	author = {Vink, Gerko},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\ZXGCXPP4\\Vink - Towards a standardized evaluation of multiple impu.pdf},
	langid = {english},
	title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
	year = {n.d.}}

@article{wong2015economic,
	author = {Wong, Frances Kam Yuet and So, Ching and Chau, June and Law, Antony Kwan Pui and Tam, Stanley Ku Fu and McGhee, Sarah},
	date-added = {2016-02-01 08:06:53 +0000},
	date-modified = {2016-02-01 08:06:53 +0000},
	journal = {Age and ageing},
	number = {1},
	pages = {143--147},
	publisher = {{Br Geriatrics Soc}},
	title = {Economic Evaluation of the Differential Benefits of Home Visits with Telephone Calls and Telephone Calls Only in Transitional Discharge Support},
	volume = {44},
	year = {2015}}

@article{wood15,
	abstract = {Multiple imputation can be used as a tool in the process of constructing prediction models in medical and epidemiological studies with missing covariate values. Such models can be used to make predictions for model performance assessment, but the task is made more complicated by the multiple imputation structure. We summarize various predictions constructed from covariates, including multiply imputed covariates, and either the set of imputation-specific prediction model coefficients or the pooled prediction model coefficients. We further describe approaches for using the predictions to assess model performance. We distinguish between ideal model performance and pragmatic model performance, where the former refers to the model's performance in an ideal clinical setting where all individuals have fully observed predictors and the latter refers to the model's performance in a real-world clinical setting where some individuals have missing predictors. The approaches are compared through an extensive simulation study based on the UK700 trial. We determine that measures of ideal model performance can be estimated within imputed datasets and subsequently pooled to give an overall measure of model performance. Alternative methods to evaluate pragmatic model performance are required and we propose constructing predictions either from a second set of covariate imputations which make no use of observed outcomes, or from a set of partial prediction models constructed for each potential observed pattern of covariate. Pragmatic model performance is generally lower than ideal model performance. We focus on model performance within the derivation data, but describe how to extend all the methods to a validation dataset.},
	author = {Wood, Angela M. and Royston, Patrick and White, Ian R.},
	doi = {10.1002/bimj.201400004},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\3TKVAKYY\\Wood et al. - 2015 - The estimation and use of predictions for the asse.pdf},
	issn = {1521-4036},
	journal = {Biometrical Journal. Biometrische Zeitschrift},
	keywords = {Analysis of Variance,Biometry,Clinical Trials as Topic,Humans,Logistic Models,Measures of model performance,Missing data,Model validation,Models; Statistical,Multiple imputation,Prediction models,Rubin's rules},
	langid = {english},
	month = jul,
	number = {4},
	pages = {614--632},
	pmcid = {PMC4515100},
	pmid = {25630926},
	title = {The Estimation and Use of Predictions for the Assessment of Model Performance Using Large Samples with Multiply Imputed Data},
	volume = {57},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1002/bimj.201400004}}

@article{xuAbilityDifferentImputation2020,
	abstract = {BACKGROUND: Incomplete data are of particular important influence in mental measurement questionnaires. Most experts, however, mostly focus on clinical trials  and cohort studies and generally pay less attention to this deficiency. We aim is to  compare the accuracy of four common methods for handling items missing from  different psychology questionnaires according to the items non-response rates.  METHOD: All data were drawn from the previous studies including the self-acceptance  scale (SAQ), the activities of daily living scale (ADL) and self-esteem scale  (RSES). SAQ and ADL dataset, simulation group, were used to compare and assess the  ability of four imputation methods which are direct deletion, mode imputation,  Hot-deck (HD) imputation and multiple imputation (MI) by absolute deviation, the  root mean square error and average relative error in missing proportions of 5, 10,  15 and 20\%. RSES dataset, validation group, was used to test the application of  imputation methods. All analyses were finished by SAS 9.4. RESULTS: The biases  obtained by MI are the smallest under various missing proportions. HD imputation  approach performed the lowest absolute deviation of standard deviation values. But  they got the similar results and the performances of them are obviously better than  direct deletion and mode imputation. In a real world situation, the respondents'  average score in complete data set was 28.22\,{$\pm$}\,4.63, which are not much different  from imputed datasets. The direction of the influence of the five factors on  self-esteem was consistent, although there were some differences in the size and  range of OR values in logistic regression model. CONCLUSION: MI shows the best  performance while it demands slightly more data analytic capacity and skills of  programming. And HD could be considered to impute missing values in psychological  investigation when MI cannot be performed due to limited circumstances.},
	author = {Xu, Xueying and Xia, Leizhen and Zhang, Qimeng and Wu, Shaoning and Wu, Mingcheng and Liu, Hongbo},
	doi = {10.1186/s12874-020-00932-0},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\574SZFYN\\Xu et al. - 2020 - The ability of different imputation methods for mi.pdf;C\:\\Users\\4216318\\Zotero\\storage\\L84YQDQ9\\s12874-020-00932-0.html},
	ids = {xuAbilityDifferentImputation2020a},
	issn = {1471-2288},
	journal = {BMC medical research methodology},
	keywords = {*Hot-deck imputation,*Imputation methods,*Mental measurement questionnaires,*Multiple imputation,Activities of Daily Living,Computer Simulation,Diagnostic Self Evaluation,Hot-deck imputation,Humans,Imputation methods,Mental Disorders/*diagnosis/psychology,Mental Health/*statistics \& numerical data,Mental measurement questionnaires,Multiple imputation,Outcome Assessment; Health Care/methods/statistics \& numerical data,Psychiatric Status Rating Scales/standards/*statistics \& numerical data,Psychometrics/methods/standards/*statistics \& numerical data,Reproducibility of Results,Surveys and Questionnaires/standards/*statistics \& numerical data},
	langid = {english},
	month = feb,
	number = {1},
	pages = {42},
	pmcid = {PMC7045426},
	pmid = {32103723},
	title = {The Ability of Different Imputation Methods for Missing Values in Mental Measurement Questionnaires.},
	volume = {20},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1186/s12874-020-00932-0}}

@article{zhao22,
	abstract = {Model checking in multiple imputation (MI, Rubin in Multiple imputation for nonresponse in surveys, Wiley, New York, 1987) becomes increasingly important with the recent developments in MI and its widespread use in statistical analysis with missing data (e.g. van Buuren et al. in J Stat Comput Simul 76(12):1049\textendash 1064, 2006; van Buuren and Groothuis-Oudshoorn in J Stat Soft 45(3):1\textendash 67, 2011; Chen et al. in Biometrics 67:799\textendash 809, 2011; Nguyen et al. in Emerg Themes Epidemiol 14(8):1\textendash 12, 2017). The currently recommended posterior predictive checking method (He and Zaslavsky in Stat Med 31:1\textendash 18, 2012; Nguyen et al. in Biom J 4:676\textendash 694, 2015) is less effective when the proportion of missing values increases and its produced posterior predictive p value is not supported by a null distribution as a standard p value (Meng in Annu Stat 22:1142\textendash 1160, 1994). This research develops a new diagnostic method for checking MI models and proposes a test statistic with a standard p value. The new diagnostic checking method is effective and flexible. It does not depend on the proportion of missing values and can deal with data sets with arbitrary nonmonotone missing data patterns. We examine the performance of the proposed method in a simulation study and illustrate the method in a study of coronary disease and associated factors.},
	author = {Zhao, Yang},
	date-modified = {2022-09-20 15:58:51 +0200},
	doi = {10.1007/s10182-021-00429-1},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\5FEMH4VT\\Zhao - 2022 - Diagnostic checking of multiple imputation models.pdf},
	issn = {1863-818X},
	journal = {AStA Advances in Statistical Analysis},
	langid = {english},
	month = jan,
	pages = {271-286},
	title = {Diagnostic Checking of Multiple Imputation Models},
	volume = {106},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10182-021-00429-1}}

@article{zhu15,
	abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
	author = {Zhu, Jian and Raghunathan, Trivellore E.},
	doi = {10.1080/01621459.2014.948117},
	file = {C\:\\Users\\4216318\\Zotero\\storage\\FPPUB4TU\\Zhu en Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf;C\:\\Users\\4216318\\Zotero\\storage\\CJWQJ3FF\\01621459.2014.html},
	journal = {Journal of the American Statistical Association},
	keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
	month = jul,
	number = {511},
	pages = {1112--1124},
	title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
	volume = {110},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2014.948117}}
