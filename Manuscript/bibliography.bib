@article{simmiss,
  author       = {Gerko Vink},
  journal = {GitHub repository. https://doi.org/10.5281/zenodo.7467995},
  title        = {Strategies for simulating missingness. https://www.gerkovink.com/simulate},
  month        = dec,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v1.0},
  doi          = {10.5281/zenodo.7467995},
  url          = {https://doi.org/10.5281/zenodo.7467995}
}

@article{bart2015,
	author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R and Alzheimer's Disease Neuroimaging Initiative*},
	journal = {Statistical methods in medical research},
	number = {4},
	pages = {462--487},
	publisher = {Sage Publications Sage UK: London, England},
	title = {Multiple imputation of covariates by fully conditional specification: accommodating the substantive model},
	volume = {24},
	year = {2015}}

@article{caiPPC,
	author = {Cai, Mingyang and {Van Buuren}, Stef and Vink, Gerko},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2208.12929},
	journal = {arXiv preprint. arXiv:2208.12929},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Graphical and numerical diagnostic tools to assess multiple imputation models by posterior predictive checking},
	url = {https://arxiv.org/abs/2208.12929},
	year = {2022},
	Bdsk-Url-1 = {https://arxiv.org/abs/2208.12929},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2208.12929}}

@article{rei03,
  title={Inference for partially synthetic, public use microdata sets},
  author={Reiter, Jerome P},
  journal={Survey Methodology},
  volume={29},
  number={2},
  pages={181--188},
  year={2003}
}

@article{bar99,
  title={Miscellanea. Small-sample degrees of freedom with multiple imputation},
  author={Barnard, John and Rubin, Donald B},
  journal={Biometrika},
  volume={86},
  number={4},
  pages={948--955},
  year={1999},
  publisher={Oxford University Press}
}

@article{amelia,
	author = {James Honaker and Gary King and Matthew Blackwell},
	journal = {Journal of Statistical Software},
	number = {7},
	pages = {1--47},
	title = {{Amelia II}: A Program for Missing Data},
	url = {https://www.jstatsoft.org/v45/i07/},
	volume = {45},
	year = {2011},
	Bdsk-Url-1 = {https://www.jstatsoft.org/v45/i07/}}

@article{robins1997,
  title={Non-response models for the analysis of non-monotone ignorable missing data},
  author={Robins, James M and Gill, Richard D},
  journal={Statistics in medicine},
  volume={16},
  number={1},
  pages={39--56},
  year={1997},
  publisher={Wiley Online Library}
}

@article{abayomi2008diagnostics,
  ids = {abay08},
  title = {Diagnostics for Multivariate Imputations},
  author = {Abayomi, Kobi and Gelman, Andrew and Levy, Marc},
  year = {2008},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {57},
  number = {3},
  pages = {273--291},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-05 21:01:33 +0000},
  date-modified = {2016-02-05 21:01:33 +0000},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YHCKE6CH\\Abayomi e.a. - 2008 - Diagnostics for multivariate imputations.pdf}
}

@misc{hei22,
  doi = {10.48550/ARXIV.2209.13358},
  url = {https://arxiv.org/abs/2209.13358},
  author = {Heinze, Georg and Boulesteix, Anne-Laure and Kammer, Michael and Morris, Tim P. and White, Ian R.},
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences, 62A01 (Primary)},
  title = {Phases of methodological research in biostatistics - building the evidence base for new methods},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{alkire2015global,
  title = {Global Access to Surgical Care: A Modelling Study},
  author = {Alkire, Blake C and Raykar, Nakul P and Shrime, Mark G and Weiser, Thomas G and Bickler, Stephen W and Rose, John A and Nutt, Cameron T and Greenberg, Sarah LM and Kotagal, Meera and Riesel, Johanna N and others},
  year = {2015},
  journal = {The Lancet Global Health},
  volume = {3},
  number = {6},
  pages = {e316--e323},
  publisher = {{Elsevier}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{allan2015determinants,
  title = {The Determinants of Care Home Closure},
  author = {Allan, Stephen and Forder, Julien},
  year = {2015},
  journal = {Health economics},
  volume = {24},
  number = {S1},
  pages = {132--145},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{ampute,
  title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure},
  shorttitle = {Generating Missing Values for Simulation Purposes},
  author = {Schouten, Rianne Margaretha and Lugtig, Peter and Vink, Gerko},
  year = {2018},
  month = oct,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {88},
  number = {15},
  pages = {2909--2930},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2018.1491577},
  abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
  langid = {english},
  keywords = {evaluation,Missing data,multiple imputation,multivariate amputation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\7UTD6JHS\\Schouten et al. - 2018 - Generating missing values for simulation purposes.pdf;C\:\\Users\\4216318\\Zotero\\storage\\KMSYBXTC\\00949655.2018.html}
}

@article{bond16,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models},
  author = {Bondarenko, Irina and Raghunathan, Trivellore},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {17},
  pages = {3007--3020},
  issn = {1097-0258},
  doi = {10.1002/sim.6926},
  abstract = {Multiple imputation has become a popular approach for analyzing incomplete data. Many software packages are available to multiply impute the missing values and to analyze the resulting completed data sets. However, diagnostic tools to check the validity of the imputations are limited, and the majority of the currently available methods need considerable knowledge of the imputation model. In many practical settings, however, the imputer and the analyst may be different individuals or from different organizations, and the analyst model may or may not be congenial to the model used by the imputer. This article develops and evaluates a set of graphical and numerical diagnostic tools for two practical purposes: (i) for an analyst to determine whether the imputations are reasonable under his/her model assumptions without actually knowing the imputation model assumptions; and (ii) for an imputer to fine tune the imputation model by checking the key characteristics of the observed and imputed values. The tools are based on the numerical and graphical comparisons of the distributions of the observed and imputed values conditional on the propensity of response. The methodology is illustrated using simulated data sets created under a variety of scenarios. The examples focus on continuous and binary variables, but the principles can be used to extend methods for other types of variables. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {congeniality,diagnostics,multiple imputation,propensity score},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\BBYIKCGA\\Bondarenko en Raghunathan - 2016 - Graphical and numerical diagnostic tools to assess.pdf;C\:\\Users\\4216318\\Zotero\\storage\\Q53PV2BZ\\sim.html}
}

@article{boulesteixIntroductionStatisticalSimulations2020,
  title = {Introduction to Statistical Simulations in Health Research},
  author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenf{\"u}hrer, J{\"o}rg and Sauerbrei, Willi},
  year = {2020},
  month = dec,
  journal = {BMJ Open},
  volume = {10},
  number = {12},
  pages = {e039921},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2020-039921},
  abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
  chapter = {Epidemiology},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  pmid = {33318113},
  keywords = {epidemiology,protocols \& guidelines,statistics \& research methods},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\HHZFFINH\\Boulesteix et al. - 2020 - Introduction to statistical simulations in health .pdf;C\:\\Users\\4216318\\Zotero\\storage\\6APLWJIV\\e039921.html}
}

@phdthesis{brand1999development,
  title = {Development, Implementation and Evaluation of Multiple Imputation Strategies for the Statistical Analysis of Incomplete Data Sets},
  author = {Brand, Jaap},
  year = {1999},
  date-added = {2016-02-05 17:56:37 +0000},
  date-modified = {2016-02-05 20:18:12 +0000},
  school = {Erasmus University Rotterdam}
}

@article{buur06,
  ids = {van2006fully},
  title = {Fully Conditional Specification in Multivariate Imputation},
  author = {Buuren, S. Van and Brand, J. P. L. and {Groothuis-Oudshoorn}, C. G. M. and Rubin, D. B.},
  year = {2006},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {76},
  number = {12},
  pages = {1049--1064},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/10629360600810434},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  date-added = {2016-02-05 17:56:23 +0000},
  date-modified = {2016-02-05 17:56:23 +0000},
  keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation},
  annotation = {\_eprint: https://doi.org/10.1080/10629360600810434},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\YQ2Q2H3J\\Buuren e.a. - 2006 - Fully conditional specification in multivariate im.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5VUVSNA2\\10629360600810434.html}
}

@book{buur18,
  title = {Flexible Imputation of Missing Data},
  author = {Van Buuren, Stef},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}}
}

@article{crowley2014flexible,
  title = {Flexible Work Options and Mothers' Perceptions of Career Harm},
  author = {Crowley, Jocelyn Elise and Kolenikov, Stanislav},
  year = {2014},
  journal = {The Sociological Quarterly},
  volume = {55},
  number = {1},
  pages = {168--195},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{dafoe2013democratic,
  title = {The Democratic Peace: {{Weighing}} the Evidence and Cautious Inference},
  author = {Dafoe, Allan and Oneal, John R and Russett, Bruce},
  year = {2013},
  journal = {International Studies Quarterly},
  volume = {57},
  number = {1},
  pages = {201--214},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@book{DarkData2020,
  title = {Dark {{Data}}},
  year = {Tue, 02/18/2020 - 12:00},
  abstract = {A practical guide to making good decisions in a world of missing data},
  isbn = {978-0-691-18237-7},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\QKB8XA26\\2020 - Dark Data.pdf;C\:\\Users\\4216318\\Zotero\\storage\\CYWCHLP6\\dark-data.html}
}

@article{de2014religiosity,
  title = {The Religiosity of Children of Immigrants and Natives in England, Germany, and the Netherlands: {{The}} Role of Parents and Peers in Class},
  author = {De Hoon, Sean and Van Tubergen, Frank},
  year = {2014},
  journal = {European sociological review},
  pages = {jcu038},
  publisher = {{Oxford Univ Press}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{dore18,
  title = {Missing {{Data}}: {{A Unified Taxonomy Guided}} by {{Conditional Independence}}},
  shorttitle = {Missing {{Data}}},
  author = {Doretti, Marco and Geneletti, Sara and Stanghellini, Elena},
  year = {2018},
  journal = {International Statistical Review},
  volume = {86},
  number = {2},
  pages = {189--204},
  issn = {1751-5823},
  doi = {10.1111/insr.12242},
  abstract = {Recent work (Seaman et al., ; Mealli \& Rubin, ) attempts to clarify the not always well-understood difference between realised and everywhere definitions of missing at random (MAR) and missing completely at random. Another branch of the literature (Mohan et al., ; Pearl \& Mohan, ) exploits always-observed covariates to give variable-based definitions of MAR and missing completely at random. In this paper, we develop a unified taxonomy encompassing all approaches. In this taxonomy, the new concept of `complementary MAR' is introduced, and its relationship with the concept of data observed at random is discussed. All relationships among these definitions are analysed and represented graphically. Conditional independence, both at the random variable and at the event level, is the formal language we adopt to connect all these definitions. Our paper covers both the univariate and the multivariate case, where attention is paid to monotone missingness and to the concept of sequential MAR. Specifically, for monotone missingness, we propose a sequential MAR definition that might be more appropriate than both everywhere and variable-based MAR to model dropout in certain contexts.},
  langid = {english},
  keywords = {conditional independence,dropout,missing data,taxonomy},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12242},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\VNFH8XKF\\Doretti et al. - 2018 - Missing Data A Unified Taxonomy Guided by Conditi.pdf;C\:\\Users\\4216318\\Zotero\\storage\\GYINQWHM\\insr.html}
}

@book{fimd,
  title = {Flexible Imputation of Missing Data},
  author = {Van Buuren, Stef},
  year = {2012},
  publisher = {{CRC press}},
  date-added = {2016-02-04 22:35:13 +0000},
  date-modified = {2016-02-04 22:35:43 +0000}
}

@article{gree12,
  title = {Transparency and Disclosure, Neutrality and Balance: Shared Values or Just Shared Words?},
  shorttitle = {Transparency and Disclosure, Neutrality and Balance},
  author = {Greenland, Sander},
  year = {2012},
  month = nov,
  journal = {J Epidemiol Community Health},
  volume = {66},
  number = {11},
  pages = {967--970},
  publisher = {{BMJ Publishing Group Ltd}},
  issn = {0143-005X, 1470-2738},
  doi = {10.1136/jech-2011-200459},
  abstract = {Values influence choice of methodology and thus influence every risk assessment and inference. To deal with this inescapable reality, we need to replace vague and unattainable calls for objectivity with more precise operational qualities. Among qualities that seem widely valued are transparency (openness) and neutrality (balance, fairness). Conformity of researchers to these qualities may be evaluated by considering whether their reports disclose key information desired by readers and whether their methodology encourages initial neutrality among hypotheses of concern. A case study is given in which two authors appearing to share these values and writing on ostensibly the same issues (disclosure and methodology) nonetheless appear to have very different concepts of what the values entail in practice. Thus, more precision is needed in explicating and implementing such values.},
  chapter = {Essay},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com.proxy.library.uu.nl/group/rights-licensing/permissions},
  langid = {english},
  pmid = {22268131},
  keywords = {Conflict of interest,disclosure,epidemiology,ethics,methodology,sociology of knowledge},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\JWF9K692\\Greenland - 2012 - Transparency and disclosure, neutrality and balanc.pdf;C\:\\Users\\4216318\\Zotero\\storage\\29EG9WI8\\967.html}
}

@article{gree17,
  title = {Invited {{Commentary}}: {{The Need}} for {{Cognitive Science}} in {{Methodology}}},
  shorttitle = {Invited {{Commentary}}},
  author = {Greenland, Sander},
  year = {2017},
  month = sep,
  journal = {American Journal of Epidemiology},
  volume = {186},
  number = {6},
  pages = {639--645},
  issn = {1476-6256},
  doi = {10.1093/aje/kwx259},
  abstract = {There is no complete solution for the problem of abuse of statistics, but methodological training needs to cover cognitive biases and other psychosocial factors affecting inferences. The present paper discusses 3 common cognitive distortions: 1) dichotomania, the compulsion to perceive quantities as dichotomous even when dichotomization is unnecessary and misleading, as in inferences based on whether a P value is "statistically significant"; 2) nullism, the tendency to privilege the hypothesis of no difference or no effect when there is no scientific basis for doing so, as when testing only the null hypothesis; and 3) statistical reification, treating hypothetical data distributions and statistical models as if they reflect known physical laws rather than speculative assumptions for thought experiments. As commonly misused, null-hypothesis significance testing combines these cognitive problems to produce highly distorted interpretation and reporting of study results. Interval estimation has so far proven to be an inadequate solution because it involves dichotomization, an avenue for nullism. Sensitivity and bias analyses have been proposed to address reproducibility problems (Am J Epidemiol. 2017;186(6):646-647); these methods can indeed address reification, but they can also introduce new distortions via misleading specifications for bias parameters. P values can be reframed to lessen distortions by presenting them without reference to a cutoff, providing them for relevant alternatives to the null, and recognizing their dependence on all assumptions used in their computation; they nonetheless require rescaling for measuring evidence. I conclude that methodological development and training should go beyond coverage of mechanistic biases (e.g., confounding, selection bias, measurement error) to cover distortions of conclusions produced by statistical methods and psychosocial forces.},
  langid = {english},
  pmid = {28938712},
  keywords = {behavioral economics,bias analysis,cognitive bias,Cognitive Science,Humans,Models; Statistical,motivated reasoning,nullism,overconfidence,Reproducibility of Results,Research Design,Selection Bias,sensitivity analysis,significance testing},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\JXDA474N\\Greenland - 2017 - Invited Commentary The Need for Cognitive Science.pdf}
}

@article{heDiagnosingImputationModels2012a,
  title = {Diagnosing Imputation Models by Applying Target Analyses to Posterior Replicates of Completed Data},
  author = {He, Yulei and Zaslavsky, Alan M.},
  year = {2012},
  month = jan,
  journal = {Statistics in Medicine},
  volume = {31},
  number = {1},
  pages = {1--18},
  issn = {1097-0258},
  doi = {10.1002/sim.4413},
  abstract = {Multiple imputation fills in missing data with posterior predictive draws from imputation models. To assess the adequacy of imputation models, we can compare completed data with their replicates simulated under the imputation model. We apply analyses of substantive interest to both datasets and use posterior predictive checks of the differences of these estimates to quantify the evidence of model inadequacy. We can further integrate out the imputed missing data and their replicates over the completed-data analyses to reduce variance in the comparison. In many cases, the checking procedure can be easily implemented using standard imputation software by treating re-imputations under the model as posterior predictive replicates. Thus, it can be applied for non-Bayesian imputation methods. We also sketch several strategies for applying the method in the context of practical imputation analyses. We illustrate the method using two real data applications and study its property using a simulation.},
  langid = {english},
  pmcid = {PMC4233994},
  pmid = {22139814},
  keywords = {Antineoplastic Agents,Bayes Theorem,Computer Simulation,Data Interpretation; Statistical,Humans,Male,Models; Statistical,Multiple Myeloma,Prognosis,Randomized Controlled Trials as Topic,Sequence Deletion,Survival Analysis},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\IUZ7LF8I\\He and Zaslavsky - 2012 - Diagnosing imputation models by applying target an.pdf}
}

@article{hoffmannMultiplicityAnalysisStrategies,
  title = {The Multiplicity of Analysis Strategies Jeopardizes Replicability: Lessons Learned across Disciplines},
  shorttitle = {The Multiplicity of Analysis Strategies Jeopardizes Replicability},
  author = {Hoffmann, Sabine and Sch{\"o}nbrodt, Felix and Elsas, Ralf and Wilson, Rory and Strasser, Ulrich and Boulesteix, Anne-Laure},
  journal = {Royal Society Open Science},
  volume = {8},
  number = {4},
  pages = {201925},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.201925},
  abstract = {For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.},
  keywords = {interdisciplinary perspective,metaresearch,open science,replicability crisis,uncertainty},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\XVK6YARR\\Hoffmann et al. - The multiplicity of analysis strategies jeopardize.pdf}
}

@article{klausch2015selection,
  title = {Selection Error in Single-and Mixed Mode Surveys of the {{Dutch}} General Population},
  author = {Klausch, Thomas and Hox, Joop and Schouten, Barry},
  year = {2015},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {178},
  number = {4},
  pages = {945--961},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{li2012imputing,
  title = {Imputing Missing Data by Fully Conditional Models: {{Some}} Cautionary Examples and Guidelines},
  author = {Li, Fan and Yu, Yaming and Rubin, Donald B},
  year = {2012},
  journal = {Duke University Department of Statistical Science Discussion Paper},
  volume = {1124},
  date-added = {2016-02-05 21:34:21 +0000},
  date-modified = {2016-02-05 21:34:21 +0000}
}

@article{li22,
  title = {Evaluating the Robustness of Targeted Maximum Likelihood Estimators via Realistic Simulations in Nutrition Intervention Trials},
  author = {Li, Haodong and Rosete, Sonali and Coyle, Jeremy and Phillips, Rachael V and Hejazi, Nima S and Malenica, Ivana and Arnold, Benjamin F and Benjamin-Chung, Jade and Mertens, Andrew and Colford Jr, John M},
  year = {2022},
  journal = {Statistics in Medicine},
  volume = {41},
  number = {12},
  pages = {2132--2165},
  publisher = {{Wiley Online Library}},
  issn = {0277-6715}
}

@book{litt20,
  title = {Statistical {{Analysis}} with {{Missing Data}}, {{Third Edition}} | {{Wiley Series}} in {{Probability}} and {{Statistics}}},
  author = {Little and Rubin},
  year = {2020},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\7J824PPV\\Statistical Analysis with Missing Data, Third Edit.pdf;C\:\\Users\\4216318\\Zotero\\storage\\V4RFI4LL\\9781119482260.html}
}

@article{little1988missing,
  title = {Missing-Data Adjustments in Large Surveys},
  author = {Little, Roderick JA},
  year = {1988},
  journal = {Journal of Business \& Economic Statistics},
  volume = {6},
  number = {3},
  pages = {287--296},
  publisher = {{Taylor \& Francis}},
  date-added = {2016-02-05 18:55:45 +0000},
  date-modified = {2016-02-05 18:55:45 +0000}
}

@article{little2012calibrated,
  title = {Calibrated {{Bayes}}, an Alternative Inferential Paradigm for Official Statistics},
  author = {Little, Roderick J},
  year = {2012},
  journal = {Journal of Official Statistics},
  volume = {28},
  number = {3},
  pages = {309},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{little2012prevention,
  title = {The Prevention and Treatment of Missing Data in Clinical Trials},
  author = {Little, Roderick J and D'Agostino, Ralph and Cohen, Michael L and Dickersin, Kay and Emerson, Scott S and Farrar, John T and Frangakis, Constantine and Hogan, Joseph W and Molenberghs, Geert and Murphy, Susan A and others},
  year = {2012},
  journal = {New England Journal of Medicine},
  volume = {367},
  number = {14},
  pages = {1355--1360},
  publisher = {{Mass Medical Soc}},
  date-added = {2016-01-31 18:40:06 +0000},
  date-modified = {2016-01-31 18:40:06 +0000}
}

@incollection{liu21,
  title = {Quality Control, Data Cleaning, Imputation},
  booktitle = {Clinical Applications of Artificial Intelligence in Real-World Data},
  author = {Liu, Dawei and Oberman, Hanne I. and Mu{\~n}oz, Johanna and Hoogland, Jeroen and Debray, Thomas P. A.},
  year = {2021},
  month = oct,
  eprint = {2110.15877},
  eprinttype = {arxiv},
  abstract = {This chapter addresses important steps during the quality assurance and control of RWD, with particular emphasis on the identification and handling of missing values. A gentle introduction is provided on common statistical and machine learning methods for imputation. We discuss the main strengths and weaknesses of each method, and compare their performance in a literature review. We motivate why the imputation of RWD may require additional efforts to avoid bias, and highlight recent advances that account for informative missingness and repeated observations. Finally, we introduce alternative methods to address incomplete data without the need for imputation.},
  archiveprefix = {arXiv},
  keywords = {62D10,G.3,I.5.1,J.3,Statistics - Methodology},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\2THQ48KE\\Liu et al. - 2021 - Quality control, data cleaning, imputation.pdf;C\:\\Users\\4216318\\Zotero\\storage\\9PK9I7SV\\2110.html}
}

@article{meal15,
  title = {Clarifying Missing at Random and Related Definitions, and Implications When Coupled with Exchangeability},
  author = {Mealli, Fabrizia and Rubin, Donald B.},
  year = {2015},
  journal = {Biometrika},
  volume = {102},
  number = {4},
  pages = {995--1000},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {We clarify the key concept of missingness at random in incomplete data analysis. We first distinguish between data being missing at random and the missingness mechanism being a missing-at-random one, which we call missing always at random and which is more restrictive. We further discuss how, in general, neither of these conditions is a statement about conditional independence. We then consider the implication of the more restrictive missing-always-at-random assumption when coupled with full unit-exchangeability for the matrix of the variables of interest and the missingness indicators: the conditional distribution of the missingness indicators for any variable that can have a missing value can depend only on variables that are always fully observed. We discuss implications of this for modelling missingness mechanisms.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\KF4JBHVR\\MEALLI and RUBIN - 2015 - Clarifying missing at random and related definitio.pdf}
}

@article{meng94,
  ids = {meng1994multiple},
  title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
  author = {Meng, Xiao-Li},
  year = {1994},
  month = nov,
  journal = {Statistical Science},
  volume = {9},
  number = {4},
  pages = {538--558},
  publisher = {{JSTOR}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177010269},
  abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  date-added = {2016-02-04 19:29:48 +0000},
  date-modified = {2016-02-04 23:48:01 +0000},
  langid = {english},
  keywords = {Congeniality,importance sampling,incomplete data,missing data,nonresponse,normalizing constants,public-use data file,randomization,self-efficiency},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\LDSDEUAS\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\4216318\\Zotero\\storage\\PHA35LD7\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MNNUQRPL\\1177010269.html}
}

@article{mice,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {{Van Buuren}, Stef and {Groothuis-Oudshoorn}, Karin},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Software},
  volume = {45},
  number = {1},
  pages = {1--67},
  publisher = {{American Statistical Association}},
  doi = {10.18637/jss.v045.i03},
  copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
  date-added = {2016-02-04 23:31:15 +0000},
  date-modified = {2016-02-04 23:32:22 +0000},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\W7DXJIVB\\Buuren en Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\4216318\\Zotero\\storage\\6GARXLG6\\v045i03.html}
}

@article{moha21,
  title = {Graphical {{Models}} for {{Processing Missing Data}}},
  author = {Mohan, Karthika and Pearl, Judea},
  year = {2021},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {534},
  pages = {1023--1037},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1874961},
  abstract = {This article reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We first examine the limitations of traditional frameworks from three different perspectives: transparency, estimability, and testability. We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are missing not at random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally, we derive testable implications for missing data models in both missing at random and MNAR categories.},
  keywords = {Graphical models,Missing data,Missing not at random,Nonignorable,Recoverability,Testability},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1874961},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ST2AY3Q4\\Mohan and Pearl - 2021 - Graphical Models for Processing Missing Data.pdf;C\:\\Users\\4216318\\Zotero\\storage\\LMRAVINF\\01621459.2021.html}
}

@article{molenberghs2008every,
  title = {Every Missingness Not at Random Model Has a Missingness at Random Counterpart with Equal Fit},
  author = {Molenberghs, Geert and Beunckens, Caroline and Sotto, Cristina and Kenward, Michael G},
  year = {2008},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {70},
  number = {2},
  pages = {371--388},
  publisher = {{Wiley Online Library}},
  date-added = {2016-02-04 19:19:43 +0000},
  date-modified = {2016-02-04 19:19:43 +0000}
}

@book{molenberghs2014handbook,
  title = {Handbook of Missing Data Methodology},
  author = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G and Tsiatis, Anastasios and Verbeke, Geert},
  year = {2014},
  publisher = {{CRC Press}},
  date-added = {2016-02-05 13:36:42 +0000},
  date-modified = {2016-02-05 13:36:42 +0000}
}

@article{more18,
  title = {Canonical {{Causal Diagrams}} to {{Guide}} the {{Treatment}} of {{Missing Data}} in {{Epidemiologic Studies}}},
  author = {{Moreno-Betancur}, Margarita and Lee, Katherine J. and Leacy, Finbarr P. and White, Ian R. and Simpson, Julie A. and Carlin, John B.},
  year = {2018},
  month = dec,
  journal = {American Journal of Epidemiology},
  volume = {187},
  number = {12},
  pages = {2705--2715},
  issn = {1476-6256},
  doi = {10.1093/aje/kwy173},
  abstract = {With incomplete data, the "missing at random" (MAR) assumption is widely understood to enable unbiased estimation with appropriate methods. While the need to assess the plausibility of MAR and to perform sensitivity analyses considering "missing not at random" (MNAR) scenarios has been emphasized, the practical difficulty of these tasks is rarely acknowledged. With multivariable missingness, what MAR means is difficult to grasp, and in many MNAR scenarios unbiased estimation is possible using methods commonly associated with MAR. Directed acyclic graphs (DAGs) have been proposed as an alternative framework for specifying practically accessible assumptions beyond the MAR-MNAR dichotomy. However, there is currently no general algorithm for deciding how to handle the missing data given a specific DAG. Here we construct "canonical" DAGs capturing typical missingness mechanisms in epidemiologic studies with incomplete data on exposure, outcome, and confounding factors. For each DAG, we determine whether common target parameters are "recoverable," meaning that they can be expressed as functions of the available data distribution and thus estimated consistently, or whether sensitivity analyses are necessary. We investigate the performance of available-case and multiple-imputation procedures. Using data from waves 1-3 of the Longitudinal Study of Australian Children (2004-2008), we illustrate how our findings can guide the treatment of missing data in point-exposure studies.},
  langid = {english},
  pmcid = {PMC6269242},
  pmid = {30124749},
  keywords = {Algorithms,Data Interpretation; Statistical,Epidemiologic Methods,Humans,Longitudinal Studies},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\4XV8YW8E\\Moreno-Betancur et al. - 2018 - Canonical Causal Diagrams to Guide the Treatment o.pdf}
}

@article{morr18,
  ids = {morrisUsingSimulationStudies2019},
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  month = may,
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods...},
  langid = {english},
  keywords = {graphics for simulation,Monte Carlo,simulation design,simulation reporting,simulation studies},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\2WCQMEXM\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\C2X36276\\Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\EPPH7J47\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\4216318\\Zotero\\storage\\IHBUZDPP\\sim.html;C\:\\Users\\4216318\\Zotero\\storage\\IKXB5QMM\\sim.html;C\:\\Users\\4216318\\Zotero\\storage\\LN3P265W\\login.html}
}

@article{neym34,
  ids = {neyman1934two},
  title = {On the {{Two Different Aspects}} of the {{Representative Method}}: {{The Method}} of {{Stratified Sampling}} and the {{Method}} of {{Purposive Selection}}},
  author = {Neyman, Jerzy},
  year = {1934},
  journal = {Journal of the Royal Statistical Society},
  volume = {97},
  number = {4},
  pages = {558--625},
  publisher = {{JSTOR}},
  doi = {10.2307/2342192},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ZMX9KJKR\\Neyman - 1934 - On the Two Different Aspects of the Representative.pdf}
}

@article{nguy17,
  ids = {nguyenModelCheckingMultiple2017a},
  title = {Model Checking in Multiple Imputation: An Overview and Case Study},
  shorttitle = {Model Checking in Multiple Imputation},
  author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
  year = {2017},
  month = dec,
  journal = {Emerging Themes in Epidemiology},
  volume = {14},
  number = {1},
  pages = {8},
  issn = {1742-7622},
  doi = {10.1186/s12982-017-0062-6},
  abstract = {Background:\hspace{0.6em} Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models. Analysis:\hspace{0.6em} In this paper, we provide an overview of currently available methods for checking imputation models. These include graphical checks and numerical summaries, as well as simulation-based methods such as posterior predictive checking. These model checking techniques are illustrated using an analysis affected by missing data from the Longitudinal Study of Australian Children. Conclusions:\hspace{0.6em} As multiple imputation becomes further established as a standard approach for handling missing data, it will become increasingly important that researchers employ appropriate model checking approaches to ensure that reliable results are obtained when using this method.},
  langid = {english},
  pmcid = {PMC5569512},
  pmid = {28852415},
  keywords = {Cross-validation,Diagnostics,Missing data,Model checking,Multiple imputation,Posterior predictive checking},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\292299YB\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\35CSCF29\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\GNUQEAQA\\Nguyen e.a. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TN7MF2IX\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TYVJUJNV\\Nguyen et al. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\4216318\\Zotero\\storage\\5E7A46Y4\\s12982-017-0062-6.html;C\:\\Users\\4216318\\Zotero\\storage\\CDXNKIZY\\s12982-017-0062-6.html;C\:\\Users\\4216318\\Zotero\\storage\\LCHBMRXB\\s12982-017-0062-6.html}
}

@article{niesslOveroptimismBenchmarkStudies2022,
  title = {Over-Optimism in Benchmark Studies and the Multiplicity of Design and Analysis Options When Interpreting Their Results},
  author = {Nie{\ss}l, Christina and Herrmann, Moritz and Wiedemann, Chiara and Casalicchio, Giuseppe and Boulesteix, Anne-Laure},
  year = {2022},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {12},
  number = {2},
  pages = {e1441},
  issn = {1942-4795},
  doi = {10.1002/widm.1441},
  abstract = {In recent years, the need for neutral benchmark studies that focus on the comparison of methods coming from computational sciences has been increasingly recognized by the scientific community. While general advice on the design and analysis of neutral benchmark studies can be found in recent literature, a certain flexibility always exists. This includes the choice of data sets and performance measures, the handling of missing performance values, and the way the performance values are aggregated over the data sets. As a consequence of this flexibility, researchers may be concerned about how their choices affect the results or, in the worst case, may be tempted to engage in questionable research practices (e.g., the selective reporting of results or the post hoc modification of design or analysis components) to fit their expectations. To raise awareness for this issue, we use an example benchmark study to illustrate how variable benchmark results can be when all possible combinations of a range of design and analysis options are considered. We then demonstrate how the impact of each choice on the results can be assessed using multidimensional unfolding. In conclusion, based on previous literature and on our illustrative example, we claim that the multiplicity of design and analysis options combined with questionable research practices lead to biased interpretations of benchmark results and to over-optimistic conclusions. This issue should be considered by computational researchers when designing and analyzing their benchmark studies and by the scientific community in general in an effort towards more reliable benchmark results. This article is categorized under: Technologies {$>$} Visualization Technologies {$>$} Data Preprocessing Technologies {$>$} Structure Discovery and Clustering},
  langid = {english},
  keywords = {benchmarking,method comparison,over-optimistic results,questionable research practices,variability of results},
  annotation = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1441},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\UAPDWNQF\\Nießl et al. - 2022 - Over-optimism in benchmark studies and the multipl.pdf;C\:\\Users\\4216318\\Zotero\\storage\\AMR4YZXI\\widm.html}
}

@article{nijm20,
  ids = {nijmanRealtimeImputationMissing2021a},
  title = {Real-Time Imputation of Missing Predictor Values in Clinical Practice},
  author = {Nijman, Steven W J and Hoogland, Jeroen and Groenhof, T Katrien J and Brandjes, Menno and Jacobs, John J L and Bots, Michiel L and Asselbergs, Folkert W and Moons, Karel G M and Debray, Thomas P A},
  year = {2020},
  month = dec,
  journal = {European Heart Journal - Digital Health},
  pages = {ztaa016},
  issn = {2634-3916},
  doi = {10.1093/ehjdh/ztaa016},
  abstract = {Abstract                            Aims               Use of prediction models is widely recommended by clinical guidelines, but usually requires complete information on all predictors, which is not always available in daily practice. We aim to describe two methods for real-time handling of missing predictor values when using prediction models in practice.                                         Methods and results               We compare the widely used method of mean imputation (M-imp) to a method that personalizes the imputations by taking advantage of the observed patient characteristics. These characteristics may include both prediction model variables and other characteristics (auxiliary variables). The method was implemented using imputation from a joint multivariate normal model of the patient characteristics (joint modelling imputation; JMI). Data from two different cardiovascular cohorts with cardiovascular predictors and outcome were used to evaluate the real-time imputation methods. We quantified the prediction model's overall performance [mean squared error (MSE) of linear predictor], discrimination (c-index), calibration (intercept and slope), and net benefit (decision curve analysis). When compared with mean imputation, JMI substantially improved the MSE (0.10 vs. 0.13), c-index (0.70 vs. 0.68), and calibration (calibration-in-the-large: 0.04 vs. 0.06; calibration slope: 1.01 vs. 0.92), especially when incorporating auxiliary variables. When the imputation method was based on an external cohort, calibration deteriorated, but discrimination remained similar.                                         Conclusions               We recommend JMI with auxiliary variables for real-time imputation of missing values, and to update imputation models when implementing them in new settings or (sub)populations.},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\HUXBXJ7Y\\Nijman et al. - 2020 - Real-time imputation of missing predictor values i.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MGSRFKTJ\\Nijman et al. - 2021 - Real-time imputation of missing predictor values i.pdf;C\:\\Users\\4216318\\Zotero\\storage\\WHJ9L9MB\\6042147.html}
}

@unpublished{ober21,
  title = {Missing the {{Point}}: {{Non-Convergence}} in {{Iterative Imputation Algorithms}}},
  shorttitle = {Missing the {{Point}}},
  author = {Oberman, Hanne I. and {van Buuren}, Stef and Vink, Gerko},
  year = {2021},
  month = oct,
  eprint = {2110.11951},
  eprinttype = {arxiv},
  abstract = {Iterative imputation is a popular tool to accommodate missing data. While it is widely accepted that valid inferences can be obtained with this technique, these inferences all rely on algorithmic convergence. There is no consensus on how to evaluate the convergence properties of the method. Our study provides insight into identifying non-convergence in iterative imputation algorithms. We found that--in the cases considered--inferential validity was achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\8QQ2TQFU\\Oberman et al. - 2021 - Missing the Point Non-Convergence in Iterative Im.pdf;C\:\\Users\\4216318\\Zotero\\storage\\TQRM4LVP\\2110.html}
}

@misc{pawe22,
  title = {Pitfalls and {{Potentials}} in {{Simulation Studies}}},
  author = {Pawel, Samuel and Kook, Lucas and Reeve, Kelly},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13076},
  eprint = {2203.13076},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {Comparative simulation studies are workhorse tools for benchmarking statistical methods, but if not performed transparently they may lead to overoptimistic or misleading conclusions. The current publication requirements adopted by statistics journals do not prevent questionable research practices such as selective reporting. The past years have witnessed numerous suggestions and initiatives to improve on these issues but little progress can be seen to date. In this paper we discuss common questionable research practices which undermine the validity of findings from comparative simulation studies. To illustrate our point, we invent a novel prediction method with no expected performance gain and benchmark it in a pre-registered comparative simulation study. We show how easy it is to make the method appear superior over well-established competitor methods if no protocol is in place and various questionable research practices are employed. Finally, we provide researchers, reviewers, and other academic stakeholders with concrete suggestions for improving the methodological quality of comparative simulation studies, most importantly the need for pre-registered simulation protocols.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\RVRASS5R\\Pawel et al. - 2022 - Pitfalls and Potentials in Simulation Studies.pdf;C\:\\Users\\4216318\\Zotero\\storage\\2P7J7YT3\\2203.html}
}

@article{pete14,
  title = {Causal {{Models}} and {{Learning}} from {{Data}}: {{Integrating Causal Modeling}} and {{Statistical Estimation}}},
  shorttitle = {Causal {{Models}} and {{Learning}} from {{Data}}},
  author = {Petersen, Maya L. and {Van der Laan}, Mark J.},
  year = {2014},
  month = may,
  journal = {Epidemiology},
  volume = {25},
  number = {3},
  pages = {418--426},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000078},
  abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
  langid = {american},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\BGQLYCY6\\Petersen and van der Laan - 2014 - Causal Models and Learning from Data Integrating .pdf;C\:\\Users\\4216318\\Zotero\\storage\\SM6PLS5Z\\Causal_Models_and_Learning_from_Data__Integrating.13.html}
}

@article{pulkki2015cumulative,
  title = {Cumulative Effect of Psychosocial Factors in Youth on Ideal Cardiovascular Health in Adulthood the Cardiovascular Risk in Young Finns Study},
  author = {{Pulkki-R{\aa}back}, Laura and Elovainio, Marko and Hakulinen, Christian and Lipsanen, Jari and Hintsanen, Mirka and Jokela, Markus and Kubzansky, Laura D and Hintsa, Taina and Serlachius, Anna and Laitinen, Tomi T and others},
  year = {2015},
  journal = {Circulation},
  volume = {131},
  number = {3},
  pages = {245--253},
  publisher = {{Am Heart Assoc}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@manual{R,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {Team, R Core},
  year = {2015},
  address = {{Vienna, Austria}},
  date-added = {2016-02-04 23:33:33 +0000},
  date-modified = {2016-02-04 23:35:08 +0000},
  organization = {{R Foundation for Statistical Computing}}
}

@article{raghunathan2003multiple,
  title = {Multiple Imputation for Statistical Disclosure Limitation},
  author = {Raghunathan, Trivellore E and Reiter, Jerome P and Rubin, Donald B},
  year = {2003},
  journal = {Journal of Official Statistics},
  volume = {19},
  number = {1},
  pages = {1},
  publisher = {{Statistics Sweden (SCB)}},
  date-added = {2016-02-05 21:45:54 +0000},
  date-modified = {2016-02-05 21:45:54 +0000}
}

@article{robi97,
  title = {Non-Response Models for the Analysis of Non-Monotone Ignorable Missing Data},
  author = {Robins, J. M. and Gill, R. D.},
  year = {1997},
  month = jan,
  journal = {Statistics in Medicine},
  volume = {16},
  number = {1-3},
  pages = {39--56},
  issn = {0277-6715},
  doi = {10.1002/(sici)1097-0258(19970115)16:1<39::aid-sim535>3.0.co;2-d},
  abstract = {We discuss a new class of ignorable non-monotone missing data models-the randomized monotone missingness (RMM) models. We argue that the RMM models represent the most general plausible physical mechanism for generating non-monotone ignorable data. We show that there exists ignorable missing data processes that are not RMM. We argue that it may therefore be inappropriate to analyse non-monotone missing data under the assumption that the missingness mechanism is ignorable, if a statistical test has rejected the hypothesis that the missing data process is RMM representable. We use RMM models to analyse data from a case-control study of the effects of radiation on breast cancer.},
  langid = {english},
  pmid = {9004382},
  keywords = {Algorithms,Breast Neoplasms,Data Interpretation; Statistical,Female,Humans,Likelihood Functions,Markov Chains,Models; Statistical,Neoplasms; Radiation-Induced,Neoplasms; Second Primary,Randomized Controlled Trials as Topic}
}

@article{rubi76,
  ids = {rubin1976inference},
  title = {Inference and {{Missing Data}}},
  author = {Rubin, Donald B.},
  year = {1976},
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  publisher = {{Biometrika Trust}},
  doi = {10.2307/2335739},
  abstract = {When making sampling distribution inferences about the parameter of the data, \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from \texttheta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  date-added = {2016-01-31 19:05:50 +0000},
  date-modified = {2016-01-31 19:05:50 +0000},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\XIZEI53J\\Rubin - 1976 - Inference and Missing Data.pdf}
}

@book{rubi87,
  ids = {rubin1987},
  title = {Multiple {{Imputation}} for Nonresponse in Surveys},
  author = {Rubin, Donald B.},
  year = {1987},
  series = {Wiley Series in Probability and Mathematical Statistics {{Applied}} Probability and Statistics},
  publisher = {{Wiley}},
  address = {{New York, NY}},
  date-added = {2016-01-31 18:37:31 +0000},
  date-modified = {2016-01-31 18:41:54 +0000},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\63UKR3PY\\Rubin - 1987 - Multiple Imputation for nonresponse in surveys.pdf}
}

@article{rubin1986statistical,
  title = {Statistical Matching Using File Concatenation with Adjusted Weights and Multiple Imputations},
  author = {Rubin, Donald B},
  year = {1986},
  journal = {Journal of Business \& Economic Statistics},
  volume = {4},
  number = {1},
  pages = {87--94},
  publisher = {{Taylor \& Francis Group}},
  date-added = {2016-02-05 18:56:47 +0000},
  date-modified = {2016-02-05 18:56:47 +0000}
}

@article{scho18,
  title = {The {{Dance}} of the {{Mechanisms}}: {{How Observed Information Influences}} the {{Validity}} of {{Missingness Assumptions}}},
  shorttitle = {The {{Dance}} of the {{Mechanisms}}},
  author = {Schouten, Rianne Margaretha and Vink, Gerko},
  year = {2021},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {50},
  number = {3},
  pages = {1243--1258},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124118799376},
  abstract = {Missing data in scientific research go hand in hand with assumptions about the nature of the missingness. When dealing with missing values, a set of beliefs has to be formulated about the extent to which the observed data may also hold for the missing parts of the data. It is vital that the validity of these missingness assumptions is verified, tested, and that assumptions are adjusted when necessary. In this article, we demonstrate how observed data structures could a priori indicate whether it is likely that our beliefs about the missingness can be trusted. To this end, we simulate complete data and generate missing values according several types of MCAR, MAR, and MNAR mechanisms. We demonstrate that in scenarios where the data correlations are either low or very substantial, strictly different mechanisms yield equivalent statistical inferences. In addition, we show that the choice of quantity of scientific interest together with the distribution of the nonresponse govern the validity of the missingness assumptions.},
  langid = {english},
  keywords = {missing data methodology,missingness assumptions,multivariate amputation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\V8JNSTN7\\Schouten and Vink - 2021 - The Dance of the Mechanisms How Observed Informat.pdf}
}

@misc{scho21,
  title = {Regression and {{Causality}}},
  author = {Schomaker, Michael},
  year = {2021},
  month = mar,
  number = {arXiv:2006.11754},
  eprint = {2006.11754},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11754},
  abstract = {The causal effect of an intervention (treatment/exposure) on an outcome can be estimated by: i) specifying knowledge about the data-generating process; ii) assessing under what assumptions a target quantity, such as for example a causal odds ratio, can be identified given the specified knowledge (and given the measured data); and then, iii) using appropriate statistical estimation techniques to estimate the desired parameter of interest. As regression is the cornerstone of statistical analysis, it seems obvious to ask: is it appropriate to use estimated regression parameters for causal effect estimation? It turns out that using regression for effect estimation is possible, but typically requires more assumptions than competing methods. This manuscript provides a comprehensive summary of the assumptions needed to identify and estimate a causal parameter using regression and, equally important, discusses the resulting implications for statistical practice.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\76MND5AX\\Schomaker - 2021 - Regression and Causality.pdf;C\:\\Users\\4216318\\Zotero\\storage\\D88BMLAW\\2006.html}
}

@article{seam13,
  title = {What {{Is Meant}} by "{{Missing}} at {{Random}}"?},
  author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
  year = {2013},
  journal = {Statistical Science},
  volume = {28},
  number = {2},
  pages = {257--268},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {The concept of missing at random is central in the literature on statistical analysis with missing data. In general, inference using incomplete data should be based not only on observed data values but should also take account of the pattern of missing values. However, it is often said that if data are missing at random, valid inference using likelihood approaches (including Bayesian) can be obtained ignoring the missingness mechanism. Unfortunately, the term "missing at random" has been used inconsistently and not always clearly; there has also been a lack of clarity around the meaning of "valid inference using likelihood". These issues have created potential for confusion about the exact conditions under which the missingness mechanism can be ignored, and perhaps fed confusion around the meaning of "analysis ignoring the missingness mechanism". Here we provide standardised precise definitions of "missing at random" and "missing completely at random", in order to promote unification of the theory. Using these definitions we clarify the conditions that suffice for "valid inference" to be obtained under a variety of inferential paradigms.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\EXRR3CWW\\Seaman et al. - 2013 - What Is Meant by Missing at Random.pdf}
}

@article{shara2015randomly,
  title = {Randomly and Non-Randomly Missing Renal Function Data in the Strong Heart Study: A Comparison of Imputation Methods},
  author = {Shara, Nawar and Yassin, Sayf A and Valaitis, Eduardas and Wang, Hong and Howard, Barbara V and Wang, Wenyu and Lee, Elisa T and Umans, Jason G},
  year = {2015},
  journal = {PloS one},
  volume = {10},
  number = {9},
  pages = {e0138923},
  publisher = {{Public Library of Science}},
  date-added = {2016-02-05 20:16:12 +0000},
  date-modified = {2016-02-05 20:16:12 +0000}
}

@article{sorbi2014medium,
  title = {Medium-Term Effectiveness of Online Behavioral Training in Migraine Self-Management: {{A}} Randomized Trial Controlled over 10 Months},
  author = {Sorbi, MJ and Kleiboer, AM and {van Silfhout}, HG and Vink, G and Passchier, J},
  year = {2014},
  journal = {Cephalalgia : an international journal of headache},
  pages = {0333102414547137},
  publisher = {{SAGE Publications}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{sper20,
  title = {Missing Data Should Be Handled Differently for Prediction than for Description or Causal Explanation},
  author = {Sperrin, Matthew and Martin, Glen P. and Sisk, Rose and Peek, Niels},
  year = {2020},
  month = sep,
  journal = {Journal of Clinical Epidemiology},
  volume = {125},
  pages = {183--187},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2020.03.028},
  abstract = {Missing data are much studied in epidemiology and statistics. Theoretical development and application of methods for handling missing data have mostly been conducted in the context of prospective research data and with a goal of description or causal explanation. However, it is now common to build predictive models using routinely collected data, where missing patterns may convey important information, and one might take a pragmatic approach to optimizing prediction. Therefore, different methods to handle missing data may be preferred. Furthermore, an underappreciated issue in prediction modeling is that the missing data method used in model development may not match the method used when a model is deployed. This may lead to overoptimistic assessments of model performance. For prediction, particularly with routinely collected data, methods for handling missing data that incorporate information within the missingness pattern should be explored and further developed. Where missing data methods differ between model development and model deployment, the implications of this must be explicitly evaluated. The trade-off between building a prediction model that is causally principled, and building a prediction model that maximizes the use of all available information, should be carefully considered and will depend on the intended use of the model.},
  langid = {english},
  pmid = {32540389},
  keywords = {Causality,Clinical prediction models,Data Interpretation; Statistical,Data Management,Humans,Missing data,Model performance,Models; Statistical,Multiple imputation,Prognosis,Prognostic model,Prospective Studies,Routinely collected data}
}

@article{stinesen2015reduced,
  title = {Reduced Vaginal Elasticity, Reduced Lubrication, and Deep and Superficial Dyspareunia in Irradiated Gynecological Cancer Survivors},
  author = {Stinesen Kollberg, Karin and Waldenstr{\"o}m, Ann-Charlotte and Bergmark, Karin and Dunberger, Gail and Rossander, Anna and Wilder{\"a}ng, Ulrica and {\AA}vall-Lundqvist, Elisabeth and Steineck, Gunnar},
  year = {2015},
  journal = {Acta Oncologica},
  volume = {54},
  number = {5},
  pages = {772--779},
  publisher = {{Taylor \& Francis}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{toCharacterizingEffectsMissing2018,
  ids = {toCharacterizingEffectsMissing2018a},
  title = {Characterizing the Effects of Missing Data and Evaluating Imputation Methods for Chemical Prioritization Applications Using {{ToxPi}}.},
  author = {To, Kimberly T. and Fry, Rebecca C. and Reif, David M.},
  year = {2018},
  journal = {BioData mining},
  volume = {11},
  pages = {10},
  issn = {1756-0381},
  doi = {10.1186/s13040-018-0169-5},
  abstract = {BACKGROUND: The Toxicological Priority Index (ToxPi) is a method for prioritization and profiling of chemicals that integrates data from diverse sources. However,  individual data sources ("assays"), such as in vitro bioassays or in vivo study  endpoints, often feature sections of missing data, wherein subsets of chemicals have  not been tested in all assays. In order to investigate the effects of missing data  and recommend solutions, we designed simulation studies around high-throughput  screening data generated by the ToxCast and Tox21 programs on chemicals highlighted  by the Agency for Toxic Substances and Disease Registry's (ATSDR) Substance Priority  List (SPL), which helps prioritize environmental research and remediation resources.  RESULTS: Our simulations explored a wide range of scenarios concerning data (0-80\%  assay data missing per chemical), modeling (ToxPi models containing from 160-700  different assays), and imputation method (k-Nearest-Neighbor, Max, Mean, Min,  Binomial, Local Least Squares, and Singular Value Decomposition). We find that most  imputation methods result in significant changes to ToxPi score, except for datasets  with a small number of assays. If we consider rank change conditional on these  significant changes to ToxPi score, we find that ranks of chemicals in the minimum  value imputation, SVD imputation, and kNN imputation sets are more sensitive to the  score changes. CONCLUSIONS: We found that the choice of imputation strategy exerted  significant influence over both scores and associated ranks, and the most sensitive  scenarios were those involving fewer assays plus higher proportions of missing data.  By characterizing the effects of missing data and the relative benefit of imputation  approaches across real-world data scenarios, we can augment confidence in the  robustness of decisions regarding the health and ecological effects of environmental  chemicals.},
  langid = {english},
  pmcid = {PMC5998548},
  pmid = {29942350},
  keywords = {Chemical prioritization,Imputation,Missing data,Multiple imputation,Simulation,ToxCast,ToxPi},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\N25MEBT5\\To et al. - 2018 - Characterizing the effects of missing data and eva.pdf;C\:\\Users\\4216318\\Zotero\\storage\\WU34E9ED\\s13040-018-0169-5.html}
}

@article{vink14,
  ids = {vink2014pooling},
  title = {Pooling Multiple Imputations When the Sample Happens to Be the Population},
  author = {Vink, Gerko and {Van Buuren}, Stef},
  year = {2014},
  month = sep,
  abstract = {Current pooling rules for multiply imputed data assume infinite populations. In some situations this assumption is not feasible as every unit in the population has been observed, potentially leading to over-covered population estimates. We simplify the existing pooling rules for situations where the sampling variance is not of interest. We compare these rules to the conventional pooling rules and demonstrate their use in a situation where there is no sampling variance. Using the standard pooling rules in situations where sampling variance should not be considered, leads to overestimation of the variance of the estimates of interest, especially when the amount of missingness is not very large. As a result, populations estimates are over-covered, which may lead to a loss of statistical power. We conclude that the theory of multiple imputation can be extended to the situation where the sample happens to be the population. The simplified pooling rules can be easily implemented to obtain valid inference in cases where we have observed essentially all units and in simulation studies addressing the missingness mechanism only.},
  date-added = {2016-01-31 18:55:15 +0000},
  date-modified = {2016-01-31 18:55:15 +0000},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9TV8WDM5\\Vink en van Buuren - 2014 - Pooling multiple imputations when the sample happe.pdf;C\:\\Users\\4216318\\Zotero\\storage\\P6KYWI5H\\1409.html}
}

@article{vinknd,
  title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
  author = {Vink, Gerko},
  year = {n.d.},
  abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\ZXGCXPP4\\Vink - Towards a standardized evaluation of multiple impu.pdf}
}

@article{volk21,
  title = {Anonymiced Shareable Data: {{Using}} Mice to Create and Analyze Multiply Imputed Synthetic Datasets},
  author = {Volker, Thom Benjamin and Vink, Gerko},
  year = {2021},
  journal = {Psych},
  volume = {3},
  number = {4},
  pages = {703--716},
  publisher = {{MDPI}},
  issn = {2624-8611}
}

@article{wong2015economic,
  title = {Economic Evaluation of the Differential Benefits of Home Visits with Telephone Calls and Telephone Calls Only in Transitional Discharge Support},
  author = {Wong, Frances Kam Yuet and So, Ching and Chau, June and Law, Antony Kwan Pui and Tam, Stanley Ku Fu and McGhee, Sarah},
  year = {2015},
  journal = {Age and ageing},
  volume = {44},
  number = {1},
  pages = {143--147},
  publisher = {{Br Geriatrics Soc}},
  date-added = {2016-02-01 08:06:53 +0000},
  date-modified = {2016-02-01 08:06:53 +0000}
}

@article{wood15,
  title = {The Estimation and Use of Predictions for the Assessment of Model Performance Using Large Samples with Multiply Imputed Data},
  author = {Wood, Angela M. and Royston, Patrick and White, Ian R.},
  year = {2015},
  month = jul,
  journal = {Biometrical Journal. Biometrische Zeitschrift},
  volume = {57},
  number = {4},
  pages = {614--632},
  issn = {1521-4036},
  doi = {10.1002/bimj.201400004},
  abstract = {Multiple imputation can be used as a tool in the process of constructing prediction models in medical and epidemiological studies with missing covariate values. Such models can be used to make predictions for model performance assessment, but the task is made more complicated by the multiple imputation structure. We summarize various predictions constructed from covariates, including multiply imputed covariates, and either the set of imputation-specific prediction model coefficients or the pooled prediction model coefficients. We further describe approaches for using the predictions to assess model performance. We distinguish between ideal model performance and pragmatic model performance, where the former refers to the model's performance in an ideal clinical setting where all individuals have fully observed predictors and the latter refers to the model's performance in a real-world clinical setting where some individuals have missing predictors. The approaches are compared through an extensive simulation study based on the UK700 trial. We determine that measures of ideal model performance can be estimated within imputed datasets and subsequently pooled to give an overall measure of model performance. Alternative methods to evaluate pragmatic model performance are required and we propose constructing predictions either from a second set of covariate imputations which make no use of observed outcomes, or from a set of partial prediction models constructed for each potential observed pattern of covariate. Pragmatic model performance is generally lower than ideal model performance. We focus on model performance within the derivation data, but describe how to extend all the methods to a validation dataset.},
  langid = {english},
  pmcid = {PMC4515100},
  pmid = {25630926},
  keywords = {Analysis of Variance,Biometry,Clinical Trials as Topic,Humans,Logistic Models,Measures of model performance,Missing data,Model validation,Models; Statistical,Multiple imputation,Prediction models,Rubin's rules},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\3TKVAKYY\\Wood et al. - 2015 - The estimation and use of predictions for the asse.pdf}
}

@article{xuAbilityDifferentImputation2020,
  ids = {xuAbilityDifferentImputation2020a},
  title = {The Ability of Different Imputation Methods for Missing Values in Mental Measurement Questionnaires.},
  author = {Xu, Xueying and Xia, Leizhen and Zhang, Qimeng and Wu, Shaoning and Wu, Mingcheng and Liu, Hongbo},
  year = {2020},
  month = feb,
  journal = {BMC medical research methodology},
  volume = {20},
  number = {1},
  pages = {42},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-00932-0},
  abstract = {BACKGROUND: Incomplete data are of particular important influence in mental measurement questionnaires. Most experts, however, mostly focus on clinical trials  and cohort studies and generally pay less attention to this deficiency. We aim is to  compare the accuracy of four common methods for handling items missing from  different psychology questionnaires according to the items non-response rates.  METHOD: All data were drawn from the previous studies including the self-acceptance  scale (SAQ), the activities of daily living scale (ADL) and self-esteem scale  (RSES). SAQ and ADL dataset, simulation group, were used to compare and assess the  ability of four imputation methods which are direct deletion, mode imputation,  Hot-deck (HD) imputation and multiple imputation (MI) by absolute deviation, the  root mean square error and average relative error in missing proportions of 5, 10,  15 and 20\%. RSES dataset, validation group, was used to test the application of  imputation methods. All analyses were finished by SAS 9.4. RESULTS: The biases  obtained by MI are the smallest under various missing proportions. HD imputation  approach performed the lowest absolute deviation of standard deviation values. But  they got the similar results and the performances of them are obviously better than  direct deletion and mode imputation. In a real world situation, the respondents'  average score in complete data set was 28.22\,{$\pm$}\,4.63, which are not much different  from imputed datasets. The direction of the influence of the five factors on  self-esteem was consistent, although there were some differences in the size and  range of OR values in logistic regression model. CONCLUSION: MI shows the best  performance while it demands slightly more data analytic capacity and skills of  programming. And HD could be considered to impute missing values in psychological  investigation when MI cannot be performed due to limited circumstances.},
  langid = {english},
  pmcid = {PMC7045426},
  pmid = {32103723},
  keywords = {*Hot-deck imputation,*Imputation methods,*Mental measurement questionnaires,*Multiple imputation,Activities of Daily Living,Computer Simulation,Diagnostic Self Evaluation,Hot-deck imputation,Humans,Imputation methods,Mental Disorders/*diagnosis/psychology,Mental Health/*statistics \& numerical data,Mental measurement questionnaires,Multiple imputation,Outcome Assessment; Health Care/methods/statistics \& numerical data,Psychiatric Status Rating Scales/standards/*statistics \& numerical data,Psychometrics/methods/standards/*statistics \& numerical data,Reproducibility of Results,Surveys and Questionnaires/standards/*statistics \& numerical data},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\574SZFYN\\Xu et al. - 2020 - The ability of different imputation methods for mi.pdf;C\:\\Users\\4216318\\Zotero\\storage\\L84YQDQ9\\s12874-020-00932-0.html}
}

@article{zhao22,
  title = {Diagnostic Checking of Multiple Imputation Models},
  author = {Zhao, Yang},
  year = {2022},
  month = jan,
  journal = {AStA Advances in Statistical Analysis},
  issn = {1863-818X},
  doi = {10.1007/s10182-021-00429-1},
  abstract = {Model checking in multiple imputation (MI, Rubin in Multiple imputation for nonresponse in surveys, Wiley, New York, 1987) becomes increasingly important with the recent developments in MI and its widespread use in statistical analysis with missing data (e.g. van Buuren et al. in J Stat Comput Simul 76(12):1049\textendash 1064, 2006; van Buuren and Groothuis-Oudshoorn in J Stat Soft 45(3):1\textendash 67, 2011; Chen et al. in Biometrics 67:799\textendash 809, 2011; Nguyen et al. in Emerg Themes Epidemiol 14(8):1\textendash 12, 2017). The currently recommended posterior predictive checking method (He and Zaslavsky in Stat Med 31:1\textendash 18, 2012; Nguyen et al. in Biom J 4:676\textendash 694, 2015) is less effective when the proportion of missing values increases and its produced posterior predictive p value is not supported by a null distribution as a standard p value (Meng in Annu Stat 22:1142\textendash 1160, 1994). This research develops a new diagnostic method for checking MI models and proposes a test statistic with a standard p value. The new diagnostic checking method is effective and flexible. It does not depend on the proportion of missing values and can deal with data sets with arbitrary nonmonotone missing data patterns. We examine the performance of the proposed method in a simulation study and illustrate the method in a study of coronary disease and associated factors.},
  langid = {english},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\5FEMH4VT\\Zhao - 2022 - Diagnostic checking of multiple imputation models.pdf}
}

@article{zhu15,
  title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
  author = {Zhu, Jian and Raghunathan, Trivellore E.},
  year = {2015},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {511},
  pages = {1112--1124},
  doi = {10.1080/01621459.2014.948117},
  abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
  keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\FPPUB4TU\\Zhu en Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf;C\:\\Users\\4216318\\Zotero\\storage\\CJWQJ3FF\\01621459.2014.html}
}

